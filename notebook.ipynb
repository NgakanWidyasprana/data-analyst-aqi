{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MRre5ju9ArGj",
        "47yR7-JVBJxt",
        "PF3TNY5YQ6N8",
        "XKBrV1Z2RGTM",
        "U-qTCnq1OKmG",
        "y16J1HglOdY0",
        "QYgx1VBwOkU1",
        "cyxGU_RiOqsl",
        "i-TDM_n3OwYn",
        "-AF0wABxO0-2",
        "iTswQgdGPEvp",
        "2rZSjI0YPMwv",
        "ZqIVBgBGPXg_",
        "kVy_TEGPPdTg",
        "agm-9q_rPktA",
        "-MVqFZseQIji",
        "U3BjlP3_RmO_",
        "UpHK4-eInoFS",
        "x2esIRV6nYfY",
        "Lt_ZoDdS4-QX",
        "V-IQYDZTsAyd",
        "_t_s2YqmBqxk",
        "1GFi3W27DLLB",
        "7byVGqBp6qFV",
        "RAuwvlj8EIYT",
        "7IK_UvJNiEHw",
        "u8o_TfKDq532",
        "9zntzO1Q_K_S",
        "kh55Tf6J_XBh",
        "9LSirkD0-vFN",
        "KIJvmKEzONG4",
        "WF1zXlX6F3D-",
        "fX3aS2gw1VFh",
        "UEebGoVaE025",
        "D5jf5b-osDvX",
        "qB7X3JANtJTZ",
        "DiIB3go7scsA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üõ´ **Proyek Analisis Data: Air Quality**\n",
        "- **Nama:** Ngakan Putu Widyasprana\n",
        "- **Email:** ngakanputu39@gmail.com\n",
        "- **ID Dicoding:** ngakanwp\n",
        "\n",
        "Data yang akan digunakan adalah Air Quality yang sumbernya dapat diakses pada tautan berikut [Air Quality](https://github.com/marceloreis/HTI/tree/master).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ANujBgB5_Put"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üôãüèª‚Äç‚ôÄÔ∏è **Pertanyaan Bisnis**\n",
        "\n"
      ],
      "metadata": {
        "id": "MRre5ju9ArGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Air Quality adalah dataset mengenai kualitas udara pada 12 daerah di beijing ([sumber informasi](https://archive.ics.uci.edu/dataset/501/beijing+multi+site+air+quality+data)) yang sudah dicocokan dengan data di China Meteorological Administration, rentang waktunya 1 Maret 2013 hingga 28 Februari 2017.\n",
        "\n",
        "**Parameter pada Dataset:**\n",
        "\n",
        "1. Month: Bulan pada saat pengukuran data (angka 1-12).\n",
        "2. Day: Hari pada saat pengukuran data (angka 1-31).\n",
        "3. Hour: Jam pada saat pengukuran data (angka 0-23).\n",
        "4. PM2.5: Konsentrasi partikel halus PM2.5 (Œºg/m¬≥). PM2.5 adalah 5 partikel udara dengan diameter ‚â§ 2.5 Œºm yang dapat membahayakan kesehatan.\n",
        "5. PM10: Konsentrasi partikel udara PM10 (Œºg/m¬≥), yaitu partikel dengan diameter ‚â§ 10 Œºm.\n",
        "6. SO2: Konsentrasi sulfur dioksida (Œºg/m¬≥), gas yang dapat berasal dari pembakaran bahan bakar fosil.\n",
        "7. NO2: Konsentrasi nitrogen dioksida (Œºg/m¬≥), gas polutan hasil pembakaran kendaraan atau industri.\n",
        "8. CO: Konsentrasi karbon monoksida (Œºg/m¬≥), gas beracun yang dihasilkan dari pembakaran tidak sempurna.\n",
        "9. O3: Konsentrasi ozon (Œºg/m¬≥), gas yang dapat membentuk kabut fotokimia dan memengaruhi kualitas udara.\n",
        "10. TEMP: Suhu udara pada saat pengukuran (¬∞C).\n",
        "11. PRES: Tekanan udara pada saat pengukuran (hPa).\n",
        "12. DEWP: Suhu titik embun (¬∞C), yaitu suhu di mana udara mencapai kejenuhan dan kondensasi terjadi.\n",
        "13. RAIN: Presipitasi atau curah hujan yang terjadi (mm).\n",
        "14. Wd: Arah angin pada saat pengukuran (kompas, seperti N, S, E, W).\n",
        "15. WSPM: Kecepatan angin (m/s).\n",
        "16. Station: Nama lokasi tempat pemantauan kualitas udara dilakukan.\n",
        "\n",
        "Pertanyaan Bisnis:\n",
        "- **Bagaimana tren polusi udara pada masing-masing daerah di Beijing?**\n",
        "- **Kapan waktu udara terparah terjadi di masing-masing daerah di Beijing?**\n",
        "- **Apakah ada korelasi antar parameter pemicu (TEMP, DEWP, RAIN, Wd, WSPM) dengan parameter polusi (PM2.5, PM10, SO2, NO2, CO, O3)? Jika ada, maka bagaimana korelasinya?**"
      ],
      "metadata": {
        "id": "bqutlh0iFR3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö **Library Data Analisis**"
      ],
      "metadata": {
        "id": "47yR7-JVBJxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy seaborn matplotlib plotly scipy"
      ],
      "metadata": {
        "id": "S5rbah1ha1Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7YNX1DbdivN"
      },
      "outputs": [],
      "source": [
        "# Library Data Retrieving and Modification\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Library for Data Preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Library for Visualization\n",
        "import scipy\n",
        "from scipy.interpolate import interp1d\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_requirements_txt():\n",
        "    \"\"\"\n",
        "    This function checks the versions of specific libraries used in the project,\n",
        "    generates a `requirements.txt` file with the required dependencies and their versions,\n",
        "    and then displays the content of that file.\n",
        "\n",
        "    Libraries checked:\n",
        "    - pandas\n",
        "    - numpy\n",
        "    - seaborn\n",
        "    - matplotlib\n",
        "    - plotly\n",
        "    - scipy\n",
        "\n",
        "    Steps performed by the function:\n",
        "    1. Print the version of each of the required libraries.\n",
        "    2. Use `pip freeze` to generate a list of installed packages and filter them to include only the required libraries.\n",
        "    3. Overwrite the `requirements.txt` file with the filtered libraries and their versions.\n",
        "    4. Display the content of the generated `requirements.txt` file in the Colab environment.\n",
        "\n",
        "    The `requirements.txt` file is saved in the current working directory of the Colab environment.\n",
        "    \"\"\"\n",
        "\n",
        "    # Print the versions of the libraries (optional, for checking)\n",
        "    print(\"pandas version:\", pd.__version__)\n",
        "    print(\"numpy version:\", np.__version__)\n",
        "    print(\"scipy version:\", scipy.__version__)\n",
        "    print(\"seaborn version:\", sns.__version__)\n",
        "    print(\"matplotlib version:\", matplotlib.__version__)\n",
        "    print(\"plotly version:\", plotly.__version__)\n",
        "\n",
        "    # Step 3: Generate `requirements.txt` for your specific libraries\n",
        "    import subprocess\n",
        "\n",
        "    # Get a list of installed libraries and their versions using pip freeze\n",
        "    installed_packages = subprocess.check_output([\"pip\", \"freeze\"]).decode(\"utf-8\").splitlines()\n",
        "\n",
        "    # List of required packages\n",
        "    required_packages = [\n",
        "        'pandas', 'numpy', 'seaborn', 'matplotlib', 'plotly', 'scipy'\n",
        "    ]\n",
        "\n",
        "    # Filter out only the required packages from the installed ones\n",
        "    filtered_packages = [\n",
        "        package for package in installed_packages if any(pkg in package for pkg in required_packages)\n",
        "    ]\n",
        "\n",
        "    # Step 4: Overwrite the `requirements.txt` file with the filtered packages\n",
        "    with open('requirements.txt', 'w') as f:  # 'w' ensures the file is overwritten\n",
        "        f.write(\"\\n\".join(filtered_packages))\n",
        "\n",
        "    # Step 5: Verify that `requirements.txt` is saved in the Colab environment\n",
        "    !cat requirements.txt\n",
        "\n",
        "generate_requirements_txt()"
      ],
      "metadata": {
        "id": "NRr9kpQAbXUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç **Data Wrangling**"
      ],
      "metadata": {
        "id": "i1QVldyyBo0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì© **Gathering Data**\n",
        "\n",
        "Sebelum memulai menjalankan kode, silahkan lakukan upload file **Air-quality-dataset.zip** untuk bisa dilakukan proses unpack."
      ],
      "metadata": {
        "id": "PF3TNY5YQ6N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unpack_zip(path):\n",
        "    \"\"\"\n",
        "    Unpacks the specified zip file to a 'content' directory.\n",
        "\n",
        "    This function checks if the given path exists. If not, it raises a\n",
        "    FileNotFoundError. It then ensures that a 'content' directory is\n",
        "    present, creating it if necessary, and extracts the zip file contents\n",
        "    into this directory.\n",
        "\n",
        "    Parameters:\n",
        "      path (str): The path to the zip file to be unpacked.\n",
        "\n",
        "    Raises:\n",
        "      FileNotFoundError: If the specified zip file does not exist.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if the path exists\n",
        "    if not os.path.exists(path): raise FileNotFoundError(f\"File '{path}' does not exist.\")\n",
        "\n",
        "    # Define the extraction destination\n",
        "    extraction_path = \"content\"\n",
        "\n",
        "    # Create the 'content' folder if it doesn't exist\n",
        "    if not os.path.exists(extraction_path): os.makedirs(extraction_path)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(path, 'r') as zip_ref: zip_ref.extractall(extraction_path)"
      ],
      "metadata": {
        "id": "Wberl-SyQYA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unpack_zip('data/Air-quality-dataset.zip')"
      ],
      "metadata": {
        "id": "I50kxrb0ld8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚è© **Insight Data Gathering**\n",
        "\n",
        "---\n",
        "\n",
        "Tidak ada insight yang bisa digali dalam sesi kali ini. Sesi kali ini berfokus untuk pengumpulan data yakni dengan cara melakukan unzip data ke folder yang sudah ditetapkan. Untuk akses data dan penilaian data dilanjutkan pada tahap berikutnya."
      ],
      "metadata": {
        "id": "oFmRxhIOGyX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üî® **Asessing Data**"
      ],
      "metadata": {
        "id": "XKBrV1Z2RGTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proses data asessing adalah proses untuk mengindentifikasi masalah pada data baik itu permasalahan data kosong, duplikat, dan data belum menggunakan format yang tepat. Pada sesi kali ini akan ada beberapa proses penilaian data, yakni sebagai berikut:\n",
        "1. Missing Column\n",
        "2. Duplicated Column\n",
        "4. Invalid Column"
      ],
      "metadata": {
        "id": "wFN1GtnwSCnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_assessing_data(df):\n",
        "    \"\"\"\n",
        "    This function assesses a DataFrame for null values, null records, duplicate rows,\n",
        "    and provides an overview of the DataFrame's structure.\n",
        "\n",
        "    Parameters:\n",
        "      df (pandas.DataFrame): The DataFrame to be assessed.\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "\n",
        "    # Check Null, Duplicated, and Info Data\n",
        "    null_data = df.isnull().sum()\n",
        "    null_records = df[df.isnull().any(axis=1)]\n",
        "    duplicate_data = df.duplicated().sum()\n",
        "\n",
        "    # Print Data\n",
        "    print(\"Null Data:\")\n",
        "    print(null_data)\n",
        "    print(\"\\nNull Records:\")\n",
        "    print(null_records)\n",
        "    print(\"\\nDuplicate Data:\")\n",
        "    print(duplicate_data)\n",
        "    print(\"\\nCheck Column Data:\")\n",
        "    df.info()"
      ],
      "metadata": {
        "id": "kruEW6S4Y5yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Data Air Quality (DAQ) - Aotizhongxin (AO)**"
      ],
      "metadata": {
        "id": "U-qTCnq1OKmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data City -- Code: AO\n",
        "\n",
        "daq_ao = pd.read_csv('content/PRSA_Data_20130301-20170228/PRSA_Data_Aotizhongxin_20130301-20170228.csv', sep=',')\n",
        "daq_ao.head()"
      ],
      "metadata": {
        "id": "1cjerSC5CfXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daq_ao.describe()"
      ],
      "metadata": {
        "id": "C1gYKn4UvtrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pengecekan apakah terdapat data kosong, jumlah record kosong, data duplikat, dan informasi data"
      ],
      "metadata": {
        "id": "aJ95xNvgTPlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_assessing_data(daq_ao)"
      ],
      "metadata": {
        "id": "4uFT3VMQFsjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Kesimpulannya Insight Data `daq_ao`:\n",
        "\n",
        "1. Terdapat 3249 record data yang null atau kosong.\n",
        "2. Tidak terdapat duplikat.\n",
        "3. Column year, month, day, dan hour bisa diubah ke tipe data datetime[ns] sehingga lebih compact dalam analisis timeseries data."
      ],
      "metadata": {
        "id": "6iWgWtCwWG78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Data Air Quality (DAQ) - Changping (CH)**"
      ],
      "metadata": {
        "id": "y16J1HglOdY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data City -- Code : CH\n",
        "\n",
        "daq_ch = pd.read_csv('content/PRSA_Data_20130301-20170228/PRSA_Data_Changping_20130301-20170228.csv', sep=',')\n",
        "daq_ch.head()"
      ],
      "metadata": {
        "id": "EByJ9jrxQP8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pengecekan apakah terdapat data kosong, jumlah record kosong, data duplikat, dan informasi data"
      ],
      "metadata": {
        "id": "bGoxH_cKbq-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_assessing_data(daq_ch)"
      ],
      "metadata": {
        "id": "epEexYOYYqKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Kesimpulannya Insight data `daq_ch`:\n",
        "1. Terdapat 2383 record data yang null atau kosong.\n",
        "2. Tidak terdapat duplikat.\n",
        "3. Column year, month, day, dan hour bisa diubah ke tipe data datetime[ns] sehingga lebih compact dalam analisis timeseries data."
      ],
      "metadata": {
        "id": "0TzXqRwNcIwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Data Air Quality (DAQ) - Dingling (DL)**"
      ],
      "metadata": {
        "id": "QYgx1VBwOkU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data City -- Code: DL\n",
        "\n",
        "daq_dl = pd.read_csv('content/PRSA_Data_20130301-20170228/PRSA_Data_Dingling_20130301-20170228.csv', sep=',')\n",
        "daq_dl.head()"
      ],
      "metadata": {
        "id": "gTaQmGU7cixf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pengecekan apakah terdapat data kosong, jumlah record kosong, data duplikat, dan informasi data"
      ],
      "metadata": {
        "id": "udVHuZxXcixg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_assessing_data(daq_dl)"
      ],
      "metadata": {
        "id": "9u5zLaJCcixg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Kesimpulannya Insight data `daq_dl`:\n",
        "1. Terdapat 3758 record data yang null atau kosong.\n",
        "2. Tidak terdapat duplikat.\n",
        "3. Column year, month, day, dan hour bisa diubah ke tipe data datetime[ns] sehingga lebih compact dalam analisis timeseries data."
      ],
      "metadata": {
        "id": "PVQmupyccixg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. Data Air Quality (DAQ) - Dongsi(DG)**"
      ],
      "metadata": {
        "id": "cyxGU_RiOqsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data City -- Code: DG\n",
        "\n",
        "daq_dg = pd.read_csv('content/PRSA_Data_20130301-20170228/PRSA_Data_Dongsi_20130301-20170228.csv', sep=',')\n",
        "daq_dg.head()"
      ],
      "metadata": {
        "id": "7VzHpvV8QRSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pengecekan apakah terdapat data kosong, jumlah record kosong, data duplikat, dan informasi data"
      ],
      "metadata": {
        "id": "XPCeUSO_dYL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_assessing_data(daq_dg)"
      ],
      "metadata": {
        "id": "9FTX1uMRdcwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Kesimpulannya Insight data `daq_dg`:\n",
        "1. Terdapat 4726 record data yang null atau kosong.\n",
        "2. Tidak terdapat duplikat.\n",
        "3. Column year, month, day, dan hour bisa diubah ke tipe data datetime[ns] sehingga lebih compact dalam analisis timeseries data."
      ],
      "metadata": {
        "id": "4XFq-ZKHdnnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5. Data Air Quality (DAQ) - Guanyuan (GY)**"
      ],
      "metadata": {
        "id": "i-TDM_n3OwYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data City -- Code: GY\n",
        "\n",
        "daq_gy = pd.read_csv('content/PRSA_Data_20130301-20170228/PRSA_Data_Guanyuan_20130301-20170228.csv', sep=',')\n",
        "daq_gy.head()"
      ],
      "metadata": {
        "id": "5LEwUn35QR4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pengecekan apakah terdapat data kosong, jumlah record kosong, data duplikat, dan informasi data"
      ],
      "metadata": {
        "id": "eyb8hoX8eJ7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_assessing_data(daq_gy)"
      ],
      "metadata": {
        "id": "2ahC4LVReLw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Kesimpulannya Insight data `daq_gy`:\n",
        "1. Terdapat 2801 record data yang null atau kosong.\n",
        "2. Tidak terdapat duplikat.\n",
        "3. Column year, month, day, dan hour bisa diubah ke tipe data datetime[ns] sehingga lebih compact dalam analisis timeseries data."
      ],
      "metadata": {
        "id": "KUrgI3qeeQ9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **6. Data Air Quality (DAQ) - Gucheng (GU)**"
      ],
      "metadata": {
        "id": "-AF0wABxO0-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data City -- Code: GU\n",
        "\n",
        "daq_gu = pd.read_csv('content/PRSA_Data_20130301-20170228/PRSA_Data_Gucheng_20130301-20170228.csv', sep=',')\n",
        "daq_gu.head()"
      ],
      "metadata": {
        "id": "v7SOtBzqQSbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pengecekan apakah terdapat data kosong, jumlah record kosong, data duplikat, dan informasi data"
      ],
      "metadata": {
        "id": "n6uXPRtLfo6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_assessing_data(daq_gu)"
      ],
      "metadata": {
        "id": "4ueq5RPXfqj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Kesimpulannya Insight data `daq_gu`:\n",
        "1. Terdapat 2560 record data yang null atau kosong.\n",
        "2. Tidak terdapat duplikat.\n",
        "3. Column year, month, day, dan hour bisa diubah ke tipe data datetime[ns] sehingga lebih compact dalam analisis timeseries data."
      ],
      "metadata": {
        "id": "2Ykh5dU4fwO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **7. Data Air Quality (DAQ) - Huairou (HU)**"
      ],
      "metadata": {
        "id": "iTswQgdGPEvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data City -- Code: HU\n",
        "\n",
        "daq_hu = pd.read_csv('content/PRSA_Data_20130301-20170228/PRSA_Data_Huairou_20130301-20170228.csv', sep=',')\n",
        "daq_hu.head()"
      ],
      "metadata": {
        "id": "-s1AHSZoQS_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pengecekan apakah terdapat data kosong, jumlah record kosong, data duplikat, dan informasi data"
      ],
      "metadata": {
        "id": "93OxU6_dgH5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_assessing_data(daq_hu)"
      ],
      "metadata": {
        "id": "8hZgdHbNgJct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Kesimpulannya Insight data `daq_hu`:\n",
        "1. Terdapat 3356 record data yang null atau kosong.\n",
        "2. Tidak terdapat duplikat.\n",
        "3. Column year, month, day, dan hour bisa diubah ke tipe data datetime[ns] sehingga lebih compact dalam analisis timeseries data."
      ],
      "metadata": {
        "id": "sP4Nkh0SgQXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **8. Data Air Quality (DAQ) - Nongzhanquan (NZ)**"
      ],
      "metadata": {
        "id": "2rZSjI0YPMwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data City -- Code: NZ\n",
        "\n",
        "daq_nz = pd.read_csv('content/PRSA_Data_20130301-20170228/PRSA_Data_Nongzhanguan_20130301-20170228.csv', sep=',')\n",
        "daq_nz.head()"
      ],
      "metadata": {
        "id": "6wF7Mk49PW6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pengecekan apakah terdapat data kosong, jumlah record kosong, data duplikat, dan informasi data"
      ],
      "metadata": {
        "id": "SSEiyZlMgi6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_assessing_data(daq_nz)"
      ],
      "metadata": {
        "id": "DH6H2-97gkyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Kesimpulannya Insight data `daq_nz`:\n",
        "1. Terdapat 1950 record data yang null atau kosong.\n",
        "2. Tidak terdapat duplikat.\n",
        "3. Column year, month, day, dan hour bisa diubah ke tipe data datetime[ns] sehingga lebih compact dalam analisis timeseries data."
      ],
      "metadata": {
        "id": "ZnBO0XvPgp-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **9. Data Air Quality (DAQ) - Shunyi (SH)**"
      ],
      "metadata": {
        "id": "ZqIVBgBGPXg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data City -- Code: SH\n",
        "\n",
        "daq_sh = pd.read_csv('content/PRSA_Data_20130301-20170228/PRSA_Data_Shunyi_20130301-20170228.csv', sep=',')\n",
        "daq_sh.head()"
      ],
      "metadata": {
        "id": "YOfLQ17tQToL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pengecekan apakah terdapat data kosong, jumlah record kosong, data duplikat, dan informasi data"
      ],
      "metadata": {
        "id": "o4vmFTK8hRI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_assessing_data(daq_sh)"
      ],
      "metadata": {
        "id": "Q7QlfOPchSyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Kesimpulannya Insight data `daq_sh`:\n",
        "1. Terdapat 4870 record data yang null atau kosong.\n",
        "2. Tidak terdapat duplikat.\n",
        "3. Column year, month, day, dan hour bisa diubah ke tipe data datetime[ns] sehingga lebih compact dalam analisis timeseries data."
      ],
      "metadata": {
        "id": "gUP1p6P_hWzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **10. Data Air Quality (DAQ) - Tiantan (TI)**"
      ],
      "metadata": {
        "id": "kVy_TEGPPdTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data City -- TI\n",
        "\n",
        "daq_ti = pd.read_csv('content/PRSA_Data_20130301-20170228/PRSA_Data_Tiantan_20130301-20170228.csv', sep=',')\n",
        "daq_ti.head()"
      ],
      "metadata": {
        "id": "REGufBXAQUOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pengecekan apakah terdapat data kosong, jumlah record kosong, data duplikat, dan informasi data"
      ],
      "metadata": {
        "id": "alGtvmYShueW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_assessing_data(daq_ti)"
      ],
      "metadata": {
        "id": "wZmnYEcIhwAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Kesimpulannya Insight data `daq_ti`:\n",
        "1. Terdapat 2221 record data yang null atau kosong.\n",
        "2. Tidak terdapat duplikat.\n",
        "3. Column year, month, day, dan hour bisa diubah ke tipe data datetime[ns] sehingga lebih compact dalam analisis timeseries data."
      ],
      "metadata": {
        "id": "NmRoLcC8h1-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **11. Data Air Quality (DAQ) - Wanliu (WL)**"
      ],
      "metadata": {
        "id": "agm-9q_rPktA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data City -- Code: WL\n",
        "\n",
        "daq_wl = pd.read_csv('content/PRSA_Data_20130301-20170228/PRSA_Data_Wanliu_20130301-20170228.csv', sep=',')\n",
        "daq_wl.head()"
      ],
      "metadata": {
        "id": "G3Iq42LDQU1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pengecekan apakah terdapat data kosong, jumlah record kosong, data duplikat, dan informasi data"
      ],
      "metadata": {
        "id": "7CJzNBWmiIjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_assessing_data(daq_wl)"
      ],
      "metadata": {
        "id": "WRGCL78ViLSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Kesimpulannya Insight data `daq_wl`:\n",
        "1. Terdapat 4430 record data yang null atau kosong.\n",
        "2. Tidak terdapat duplikat.\n",
        "3. Column year, month, day, dan hour bisa diubah ke tipe data datetime[ns] sehingga lebih compact dalam analisis timeseries data."
      ],
      "metadata": {
        "id": "SW_ddn2viSVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **12. Data Air Quality (DAQ) - Wanshouxigong (WX)**"
      ],
      "metadata": {
        "id": "-MVqFZseQIji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data City -- Code: WX\n",
        "\n",
        "daq_wx = pd.read_csv('content/PRSA_Data_20130301-20170228/PRSA_Data_Wanshouxigong_20130301-20170228.csv', sep=',')\n",
        "daq_wx.head()"
      ],
      "metadata": {
        "id": "lCRVudczQVcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pengecekan apakah terdapat data kosong, jumlah record kosong, data duplikat, dan informasi data"
      ],
      "metadata": {
        "id": "gpng03IDipZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_assessing_data(daq_wx)"
      ],
      "metadata": {
        "id": "Jm8iDxe2iq15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Kesimpulannya Insight data `daq_wx`:\n",
        "1. Terdapat 2296 record data yang null atau kosong.\n",
        "2. Tidak terdapat duplikat.\n",
        "3. Column year, month, day, dan hour bisa diubah ke tipe data datetime[ns] sehingga lebih compact dalam analisis timeseries data."
      ],
      "metadata": {
        "id": "uj_CzjzniwG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚è© **Insight Asessing Data**\n",
        "\n",
        "---\n",
        "\n",
        "| Data Name | Null Value | Duplicate Data | Invalid Format |  \n",
        "|----------|----------|----------|----------|\n",
        "| AO | 3249 | 0 | Date Invalid |\n",
        "| CH | 2383 | 0 | Date Invalid |\n",
        "| DL | 3758 | 0 | Date Invalid |\n",
        "| DG | 4726 | 0 | Date Invalid |\n",
        "| GY | 2801 | 0 | Date Invalid |\n",
        "| GU | 2560 | 0 | Date Invalid |\n",
        "| HU | 3356 | 0 | Date Invalid |\n",
        "| NZ | 1950 | 0 | Date Invalid |\n",
        "| SH | 4870 | 0 | Date Invalid |\n",
        "| TI | 2221 | 0 | Date Invalid |\n",
        "| WL | 4430 | 0 | Date Invalid |\n",
        "| WX | 2296 | 0 | Date Invalid |\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusion Statement -- Insight Asessing Data:**\n",
        "1. Dikarenakan jumah nilai *null* atau kosong yang sangat tinggi, maka jika memilih opsi untuk menghilangkan atau menghapus data akan menjadi kehilangan informasi data yang besar. Oleh karena itu, nilai *null* akan dilakukan penghitungan dan pengisian dengan **metode interpolarisasi** untuk mengisi data kosong.\n",
        "\n",
        "2. Untuk mengatasi format tanggal yang belum sesuai nanti akan dilakukan proses penggabungan dan penyesuaian data dengan format [YY-MM-DD\n",
        " HH:mm].\n",
        "\n",
        "3. Dapat dilihat juga semua data baik dari parameter polusi dan parameter pemicu polusi pada masing-masing daerah terdapat yang hilang atau missing dengan jumlah yang cukup banyak. Untuk analisis lanjutan nanti seperti pengecekan data *outlier* dan data *correlation* atau korelasi antar data akan dilakukan di segmen \"**Further Asessing Data and Cleaning**\" di proses Data Cleaning."
      ],
      "metadata": {
        "id": "boDKaUl5Xj3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üßπ **Data Cleaning**"
      ],
      "metadata": {
        "id": "U3BjlP3_RmO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proses data cleaning kali ini akan dilakukan sesuai dengan hasil proses asessing data sebelumnya. Dalam prosesnya nanti akan dibagi menjadi 3 tahapan proses:\n",
        "\n",
        "1. Proses mengubah kolom tahun, bulan, hari, dan jam menjadi satu kolom baru bernama datetime.\n",
        "2. Proses untuk mengatasi data *null* atau kosong.\n",
        "3. Proses lanjutan setelah 2 proses dijalankan yakni mengatasi data *outlier* dan pengecekan korelasi antar kolom.\n",
        "\n",
        "Dari ketiga proses tersebut nantinya akan diambil sebagai insight dan dilanjutkan ke tahap ke-4 yakni EDA (Exploratory Data Analysis)."
      ],
      "metadata": {
        "id": "USmfKJ4Ngpy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Deal with Invalid Date Format - Merge and Compact**"
      ],
      "metadata": {
        "id": "UpHK4-eInoFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_date(df):\n",
        "    \"\"\"\n",
        "    Combine year, month, day, and hour columns in a DataFrame into a single datetime column.\n",
        "\n",
        "    This function assumes the DataFrame contains columns named 'year', 'month', 'day', and 'hour',\n",
        "    and these columns contain integer values. It combines them into a new column called 'datetime'\n",
        "    in the format YYYY-MM-DD HH:00:00.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The input DataFrame with separate 'year', 'month', 'day', and 'hour' columns.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The original DataFrame with an added 'datetime' column.\n",
        "    \"\"\"\n",
        "    # Convert year, month, day, and hour column into new datetime column\n",
        "    df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
        "    return df"
      ],
      "metadata": {
        "id": "KdbBhiPHn0Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_daq = [\n",
        "    daq_ao, daq_ch, daq_dl, daq_dg,\n",
        "    daq_gy, daq_gu, daq_hu, daq_nz,\n",
        "    daq_sh, daq_ti, daq_wl, daq_wx\n",
        "]\n",
        "\n",
        "all_daq = [combine_date(df) for df in all_daq]"
      ],
      "metadata": {
        "id": "VY7hjapypx-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_daq[0]['datetime']"
      ],
      "metadata": {
        "id": "F0-SlVNqOl3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Deal with Null Value - Interpolation**"
      ],
      "metadata": {
        "id": "x2esIRV6nYfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolation_linear(df):\n",
        "    \"\"\"\n",
        "    Perform linear interpolation for numeric columns and forward/backward fill for object columns.\n",
        "\n",
        "    This function assumes the DataFrame contains a 'datetime' column, which is set as the index.\n",
        "    It also drops the 'No', 'year', 'month', 'day', and 'hour' columns if present, and performs\n",
        "    the following:\n",
        "    - For numeric columns (float64 or int64), missing values are interpolated using the 'linear' method.\n",
        "    - For object columns, missing values are filled using forward-fill (ffill) and backward-fill (bfill).\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The input DataFrame with a 'datetime' column and potential missing values.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The DataFrame with missing values handled according to column type.\n",
        "    \"\"\"\n",
        "    # Set datetime as Index First\n",
        "    df.set_index('datetime', inplace=True)\n",
        "\n",
        "    # Drop column that are not related anymore in DataFrame\n",
        "    df.drop(['No', 'year', 'month', 'day', 'hour'], axis=1, inplace=True)\n",
        "\n",
        "    # Do looping for column 'numeric'-- Linear or 'object'--ffill() and bfill()\n",
        "    # column to use different method of interpolation\n",
        "    for column in df.columns:\n",
        "      if df[column].dtype in ['float64', 'int64']:\n",
        "\n",
        "        df[column] = df[column].ffill().bfill()\n",
        "\n",
        "        if df[column].isna().any():\n",
        "            df[column] = df[column].interpolate(method='linear')\n",
        "\n",
        "      elif df[column].dtype == 'object':\n",
        "          df[column] = df[column].ffill().bfill()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "U_b94icE5A_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_daq = [\n",
        "    daq_ao, daq_ch, daq_dl, daq_dg,\n",
        "    daq_gy, daq_gu, daq_hu, daq_nz,\n",
        "    daq_sh, daq_ti, daq_wl, daq_wx\n",
        "]\n",
        "\n",
        "all_daq = [interpolation_linear(df) for df in all_daq]"
      ],
      "metadata": {
        "id": "LjfPgBRMRqaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop all to check the result\n",
        "\n",
        "# Define the DataFrame Name\n",
        "daq_names = [\n",
        "    \"daq_ao\", \"daq_ch\", \"daq_dl\", \"daq_dg\",\n",
        "    \"daq_gy\", \"daq_gu\", \"daq_hu\", \"daq_nz\",\n",
        "    \"daq_sh\", \"daq_ti\", \"daq_wl\", \"daq_wx\"\n",
        "]\n",
        "\n",
        "# Make Variabel to Store Missing Value\n",
        "missing_values = {}\n",
        "\n",
        "# Loop Trough All Dataframe and Insert in missing_values\n",
        "for name, df in zip(daq_names, all_daq):\n",
        "    missing_values[name] = df.isna().sum()\n",
        "\n",
        "# Convert and Transpose the missing_values\n",
        "missing_values_df = pd.DataFrame(missing_values)\n",
        "missing_values_df = missing_values_df.T\n",
        "\n",
        "# Reset Index and Rename 'Index' into 'DataFrame'\n",
        "missing_values_df.reset_index(inplace=True)\n",
        "missing_values_df.rename(columns={\"index\": \"DataFrame\"}, inplace=True)\n",
        "\n",
        "missing_values_df"
      ],
      "metadata": {
        "id": "sUBpcWO0xWYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_daq[0].head()"
      ],
      "metadata": {
        "id": "KotdKygo3l34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Further Asessing and Cleaning Data**"
      ],
      "metadata": {
        "id": "Lt_ZoDdS4-QX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah berhasil dalam menangani masalah pada data yakni *Invalid Date Format* dan *Null Value* selanjutnya adalah menangani masalah lanjutan seperti data *outlier* dan mengecek korelasi antar data.\n",
        "\n",
        "Akan tetapi sebelum itu, pastikan sekali lagi semua format data baik itu `int64`, `datetime[ns]`, `float64`, dan `object` sudah benar. Salah satu contohnya pada tipe data `float64` yang data seharusnya berupa angka desimal seperti 0.0, 800.0, 500.0 dan lainnya."
      ],
      "metadata": {
        "id": "5uTNvEvsT3eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_correct_dtypes(df):\n",
        "    \"\"\"\n",
        "    Ensures all columns in a list of DataFrames have the correct data types.\n",
        "\n",
        "    This function checks each DataFrame and performs the following operations:\n",
        "    - If the index is of datetime type, it is reset and moved to a regular column.\n",
        "    - Numeric columns are converted to numeric, coercing errors (i.e., invalid values become NaN).\n",
        "    - Datetime columns (including columns with 'date' in their name) are converted to datetime, coercing errors.\n",
        "    - Object (string) columns are converted to strings.\n",
        "\n",
        "    Parameters:\n",
        "    - dfs (list of pd.DataFrame): A list of DataFrames to be processed.\n",
        "\n",
        "    Returns:\n",
        "    - list of pd.DataFrame: A list of DataFrames with corrected data types.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define variabel for dataframe that already corrected\n",
        "    corrected_dataframes = []\n",
        "\n",
        "    # Looping data in list of dataframe and correct all data into\n",
        "    # corresponding data type column.\n",
        "    for data in df:\n",
        "        data = data.copy()\n",
        "\n",
        "        # Reset index datetime to regular column\n",
        "        if pd.api.types.is_datetime64_any_dtype(data.index):\n",
        "            data.reset_index(inplace=True)\n",
        "\n",
        "        # Looping all column in dataframe\n",
        "        for col in data.columns:\n",
        "\n",
        "            # Check and correct data in column numeric\n",
        "            if pd.api.types.is_numeric_dtype(data[col]):\n",
        "                data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "            # Check and correct data in column datetime\n",
        "            elif pd.api.types.is_datetime64_any_dtype(data[col]) or 'date' in col.lower():\n",
        "                data[col] = pd.to_datetime(data[col], errors='coerce')\n",
        "\n",
        "            # Check and correct data in column object\n",
        "            elif pd.api.types.is_object_dtype(data[col]):\n",
        "                data[col] = data[col].astype(str)\n",
        "\n",
        "        # Store all corrected column into variabel corrected_dataframes\n",
        "        corrected_dataframes.append(data)\n",
        "\n",
        "    return corrected_dataframes"
      ],
      "metadata": {
        "id": "r_onBDW7-k7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_df = ensure_correct_dtypes(all_daq)"
      ],
      "metadata": {
        "id": "eE9E5JLB9IDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_df[0].info()"
      ],
      "metadata": {
        "id": "DqYRpSJqAQuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_df[0]"
      ],
      "metadata": {
        "id": "zxMS-i4dAVyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üß∂ **Check and Deal With the Outlier Data**"
      ],
      "metadata": {
        "id": "V-IQYDZTsAyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_outliers_iqr(df):\n",
        "    \"\"\"\n",
        "    Detects outliers in a DataFrame using the Interquartile Range (IQR) method.\n",
        "\n",
        "    This function identifies outliers based on the IQR, which is the\n",
        "    range between the 25th and 75th percentiles (Q1 and Q3) of the data.\n",
        "    Any data points that are below the lower bound (Q1 - 1.5 * IQR) or above the upper\n",
        "    bound (Q3 + 1.5 * IQR) are considered outliers.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The input DataFrame containing numerical data for outlier detection.\n",
        "\n",
        "    Returns:\n",
        "    - outliers_df (pd.DataFrame): A DataFrame containing the rows with outliers.\n",
        "    - non_outliers_df (pd.DataFrame): A DataFrame containing the rows without outliers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define outliers variabel and select just numeric column\n",
        "    # in DataFrame\n",
        "    outliers = pd.DataFrame(index=df.index)\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "    # Loop through the all numeric column and calculate\n",
        "    # IQR (Interquartile) Method\n",
        "    for col in numeric_cols:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        # Select data that related to lower bound of data and\n",
        "        # upper bound of data\n",
        "        outliers[col] = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
        "\n",
        "    # Select data with outliers act as filter to seperated\n",
        "    # outliers and non-outliers data\n",
        "    outliers_df = df[outliers.any(axis=1)]\n",
        "    non_outliers_df = df[~outliers.any(axis=1)]\n",
        "\n",
        "    return outliers_df, non_outliers_df"
      ],
      "metadata": {
        "id": "OIfv4j1mtdV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_boxplots(df):\n",
        "    \"\"\"\n",
        "    Plots boxplots for all numeric columns in the given DataFrame.\n",
        "\n",
        "    This function generates a boxplot for each numeric column in the DataFrame,\n",
        "    displaying the distribution, median, and potential outliers. It uses `matplotlib`\n",
        "    to create the plots and adds titles and labels for clarity.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The input DataFrame containing numeric data.\n",
        "\n",
        "    Returns:\n",
        "    - None: The function displays boxplots but does not return any values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Select data that just numeric column in dataframe\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "    # Check the numeric column is not empty or 0\n",
        "    if len(numeric_cols) == 0:\n",
        "        print(\"No numeric columns to plot.\")\n",
        "        return\n",
        "\n",
        "    # Looping through numeric column and do box-plot visualization\n",
        "    for col in numeric_cols:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.boxplot(df[col].dropna(), vert=False, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
        "        plt.title(f'Box Plot for {col}', fontsize=16)\n",
        "        plt.xlabel(col, fontsize=14)\n",
        "        plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "CNZi9NMOt4z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def winsorize_outliers(df, lower_percentile=5, upper_percentile=95):\n",
        "    \"\"\"\n",
        "    Winsorizes outliers in the numeric columns of a DataFrame.\n",
        "\n",
        "    This function replaces the outliers in each numeric column with values\n",
        "    at the specified lower and upper percentiles. Outliers are defined as\n",
        "    values that are below the lower percentile or above the upper percentile.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The input DataFrame containing numeric data.\n",
        "    - lower_percentile (int, optional): The lower percentile threshold (default is 5).\n",
        "    - upper_percentile (int, optional): The upper percentile threshold (default is 95).\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: A DataFrame with winsorized values for outliers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get numeric column from dataframe\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "    # Calculate lower bound and upper bound\n",
        "    # in every numeric columns in dataframe\n",
        "    lower_bound = df[numeric_cols].quantile(lower_percentile / 100)\n",
        "    upper_bound = df[numeric_cols].quantile(upper_percentile / 100)\n",
        "\n",
        "    # Make dataframe to store result of winsorized\n",
        "    df_winsorized = df.copy()\n",
        "\n",
        "    # Looping through the numeric columns and do winsorized data in dataframe\n",
        "    for col in numeric_cols:\n",
        "        df_winsorized[col] = df_winsorized[col].clip(lower=lower_bound[col], upper=upper_bound[col])\n",
        "\n",
        "    return df_winsorized"
      ],
      "metadata": {
        "id": "xNmC6uiX0suv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set index for dataframe after winsorized\n",
        "# will be choose index 0 until 11\n",
        "index_winsorized=0"
      ],
      "metadata": {
        "id": "uzi-E5CFCGq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Check Outlier Data - 1 DataFrame Example**"
      ],
      "metadata": {
        "id": "_t_s2YqmBqxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_boxplots(corrected_df[index_winsorized])"
      ],
      "metadata": {
        "id": "jg3k7qT2ug95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_df[index_winsorized].describe()"
      ],
      "metadata": {
        "id": "ynja1QMrsO8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers, non_outliers = detect_outliers_iqr(corrected_df[index_winsorized])\n",
        "outliers.describe()"
      ],
      "metadata": {
        "id": "QgGhxK6ewgsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_outliers.describe()"
      ],
      "metadata": {
        "id": "MPaPXtNFwzMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "winsorized_df = winsorize_outliers(corrected_df[index_winsorized], lower_percentile=10, upper_percentile=90)\n",
        "winsorized_df.describe(include='all')"
      ],
      "metadata": {
        "id": "ZgXJ0RLd0-6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_boxplots(winsorized_df)"
      ],
      "metadata": {
        "id": "8_OEBX8618Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Deal Outliers Data - All DataFrame**"
      ],
      "metadata": {
        "id": "1GFi3W27DLLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_winsorized_df = [winsorize_outliers(df, lower_percentile=10, upper_percentile=90) for df in corrected_df]"
      ],
      "metadata": {
        "id": "MYb_2ouNDVbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### üß∂ **Check The Correlation**"
      ],
      "metadata": {
        "id": "7byVGqBp6qFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_correlation(df):\n",
        "    \"\"\"\n",
        "    Plots a correlation matrix heatmap for the numeric columns of a DataFrame.\n",
        "\n",
        "    This function calculates the Pearson correlation coefficient between each\n",
        "    pair of numeric columns, excluding specified non-numeric columns, and\n",
        "    visualizes the correlation matrix as a heatmap. Correlation coefficients\n",
        "    range from -1 to 1, with values closer to 1 or -1 indicating stronger correlations.\n",
        "    - Positive values indicate a positive correlation.\n",
        "    - Negative values indicate a negative correlation.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The input DataFrame containing numeric and non-numeric data.\n",
        "\n",
        "    Returns:\n",
        "    - None: This function only displays the heatmap and does not return any values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Drop column that not will use in make correlation matrix\n",
        "    df_cor = df.drop(columns=['station', 'wd', 'datetime'])\n",
        "\n",
        "    # Get correlation value and store in correlation variabel\n",
        "    correlation_matrix = df_cor.corr()\n",
        "\n",
        "    # Visualize the correlation value into correlation matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Correlation Matrix Heatmap')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2sTaQsb4Ebew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose and set index to choose dataframe\n",
        "# that will visualize from 0-11\n",
        "index_correlation = 0"
      ],
      "metadata": {
        "id": "Hi_ahpsBEnVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Check Correlation Data - 1 DataFrame Example**"
      ],
      "metadata": {
        "id": "RAuwvlj8EIYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_correlation(all_winsorized_df[index_correlation])"
      ],
      "metadata": {
        "id": "8gybv04i7A-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚è© **Insight Data Cleaning**\n",
        "\n",
        "---\n",
        "\n",
        "Setelah melakukan proses data cleaning dimulai dari proses mengubah format date menjadi lebih *compact*, mengatasi nilai *null* atau kosong, melakukan analisis lanjutan mengenai data *outlier* dan korelasi data didapatkan beberapa hal menarik sebagai berikut:\n",
        "\n",
        "1. Pengubahan format tanggal membuat kolom menjadi lebih ringkas dan membantu mempermudah dalam analisis lanjutan, seperti digunakan saat proses menyelesaikan *null* value dengan membuat kolom datetime menjadi index dari datanya.\n",
        "\n",
        "2. Menyelesaikan masalah *null* value sudah dapat mengatasi kekosongan data pada data yang bertipe numerik dan object. Untuk tipe numerik menggunakan metode interpolasi linear sedangkan untuk data tipe object menggunakan metode interpolasi *forward fill* digabungkan dengan *backward fill*.\n",
        "\n",
        "3. Setelah data lengkap, analisis lanjutan yang bisa dilakukan adalah mengatasi data *outlier* dan pengecekan korelasi antar datanya. Untuk data outlier diatasi dengan metode winsoring untuk menggantikan nilai data berdasarkan dengan batas bawah dan batas atas percentil-nya. Dengan ini maka data *outlier* sudah dapat ditangai dengan baik, ini dapat dilihat dari visualisasi box-plot yang tidak menunjukan indikasi adanya data *outlier*.\n",
        "\n",
        "4. Proses untuk pengecekan korelasi antara kolom juga dapat dilihat pada hasil *correlation matrix*. Pada matrik terdapat beberapa kolom, baik kolom parameter polusi dan kolom parameter pemicu polusi berkaitan satu sama lain walau kecil, untuk analisis lebih lanjut akan pada proses EDA nantinya.  "
      ],
      "metadata": {
        "id": "dpBgGG2ucDqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚åö **EDA (Exploratory Data Analysis)**"
      ],
      "metadata": {
        "id": "7IK_UvJNiEHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA atau Exploratory Data Analysis adalah proses untuk menggali insight dari data yang digunakan untuk menjawab pertanyaan bisnis. Dalam proses EDA terdapat beberapa proses yakni tahap persiapan dan proses eksplorasi data."
      ],
      "metadata": {
        "id": "E3uR5AcniKAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Tahap Persiapan EDA**"
      ],
      "metadata": {
        "id": "u8o_TfKDq532"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_dfs = all_winsorized_df[:12]\n",
        "final_df = pd.concat(selected_dfs, ignore_index=True)\n",
        "final_df"
      ],
      "metadata": {
        "id": "mhDJQO2HiJpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_polution_final_df = final_df.drop(columns=['TEMP','PRES','DEWP','RAIN','wd','WSPM'])\n",
        "pc_polution_final_df = final_df.drop(columns=['PM2.5','PM10','SO2','NO2','CO','O3'])"
      ],
      "metadata": {
        "id": "h_oymtZ-5GYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_polution_final_df.head()"
      ],
      "metadata": {
        "id": "7mq19Pi1-yDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc_polution_final_df.head()"
      ],
      "metadata": {
        "id": "-1m4YneX-73o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.to_csv(\"dashboard/final_df.csv\", index=False)"
      ],
      "metadata": {
        "id": "zEIJ3n7EV5aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Tahap Eksplorasi Data**"
      ],
      "metadata": {
        "id": "73owP0mOro_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ü•® **Parameter Polusi**"
      ],
      "metadata": {
        "id": "9zntzO1Q_K_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameter polusi terdiri dari beberapa kolom yakni PM2.5, PM10, SO2, NO2, CO, dan O3. Keenam kolom ini menandakan bahwa pada suatu daerah memiliki kondisi udara yang baik atau tidak."
      ],
      "metadata": {
        "id": "EeN1sF2vy-_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_polution_final_df.describe()"
      ],
      "metadata": {
        "id": "5y0mmR3hWZ3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_polution_final_df.head()"
      ],
      "metadata": {
        "id": "t2ZTEMETjQxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_polution_final_df.groupby(by=['station']).agg({\n",
        "    'PM2.5': 'mean',\n",
        "    'PM10': 'mean',\n",
        "    'SO2': 'mean',\n",
        "    'NO2': 'mean',\n",
        "    'CO': 'mean',\n",
        "    'O3': 'mean',\n",
        "}).reset_index()"
      ],
      "metadata": {
        "id": "9UP9SlSWWzrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat pada tabel diatas, terdapat total 12 kota di Beijing dengan 6 indikator kualitas dari udara nya. Secara sekilas, untuk persebaran nilai indikatornya cukup merata pada satu daerah dengan daerah yang lain, oleh karena itu, coba untuk mencari nilai maksimal dan minimal tiap indikator pada masing-masing daerah."
      ],
      "metadata": {
        "id": "NJ7aG19szWqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in p_polution_final_df.columns:\n",
        "    if col not in ['station', 'datetime']:\n",
        "      group_data = p_polution_final_df.groupby(by=['station'])[col].mean()\n",
        "      max_station = group_data.idxmax()\n",
        "      max_value = group_data.max()\n",
        "      print(f\"Kota Dengan Nilai {col} Tertinggi : {max_station}, Nilai: {max_value}\")\n",
        "\n",
        "      min_station = group_data.idxmin()\n",
        "      min_value = group_data.min()\n",
        "      print(f\"Kota Dengan Nilai {col} Terendah : {min_station}, Nilai: {min_value}\\n\")"
      ],
      "metadata": {
        "id": "RJEpkphRwzW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari 6 indikator, yang memegang posisi 3 indikator tertinggi adalah daerah Dongsi dengan indikator PM2.5, SO2, dan CO tertinggi. Sedangkan daerah yang memegang indikator terrendah terbanyak adalah Dingling, dengan 4 indikatornya adalah PM2.5, PM10, NO2, dan CO.\n"
      ],
      "metadata": {
        "id": "vpBUPn9Iz_mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_df = p_polution_final_df.copy()\n",
        "daily_df.set_index('datetime', inplace=True)\n",
        "daily_mean = daily_df.groupby('station').resample('D').mean().reset_index()\n",
        "daily_mean.head()"
      ],
      "metadata": {
        "id": "4MUJEOY4yksD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_df = p_polution_final_df.copy()\n",
        "weekly_df.set_index('datetime', inplace=True)\n",
        "weekly_mean = weekly_df.groupby('station').resample('W').mean().reset_index()\n",
        "weekly_mean.head()"
      ],
      "metadata": {
        "id": "4wi-rjZi4AZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "montly_df = p_polution_final_df.copy()\n",
        "montly_df.set_index('datetime', inplace=True)\n",
        "montly_mean = montly_df.groupby('station').resample('ME').mean().reset_index()\n",
        "montly_mean.head()"
      ],
      "metadata": {
        "id": "QzNE-n0Z4HKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annual_df = p_polution_final_df.copy()\n",
        "annual_df.set_index('datetime', inplace=True)\n",
        "annual_mean = annual_df.groupby('station').resample('YE').mean().reset_index()\n",
        "annual_mean.head()"
      ],
      "metadata": {
        "id": "XfY1k_vC4asL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah mengidentifikasi secara kasar mengenai daerah dengan indikator tertinggi dan terendah, dalam data parameter polusi juga dapat diubah menjadi rata-rata data dalam satuan jam, hari, minggu, bulan dan tahun. Ini memberikan ruang analisis menjadi lebih beragam pada skala yang lebih kecil."
      ],
      "metadata": {
        "id": "Hj-MQO_P3MYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ü•® **Parameter Pemicu Polusi**"
      ],
      "metadata": {
        "id": "kh55Tf6J_XBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameter pemicu polusi adalah beberapa parameter pendukung yang dapat memicu sebuah polusi seperti temperatur (TEMP), tekanan udara(PRES), titik embun (DEWP), tingkat curah hujan (RAIN), arah angin (wd), dan kecepatan angin (WSP)."
      ],
      "metadata": {
        "id": "kwuiKfyB41MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pc_polution_final_df.describe(include='all')"
      ],
      "metadata": {
        "id": "plizeUq-ofpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc_polution_final_df.head(10)"
      ],
      "metadata": {
        "id": "k99jEKlf5mOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc_polution_final_df.groupby(by='wd').agg({\n",
        "    'station' : 'nunique',\n",
        "    'TEMP': 'mean',\n",
        "    'PRES': 'mean',\n",
        "    'DEWP': 'mean',\n",
        "    'RAIN': 'mean',\n",
        "    'WSPM': 'mean',\n",
        "}).reset_index()"
      ],
      "metadata": {
        "id": "XeAp7CL66DwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat pada tabel, untuk arah mata angin terdapat pada semua daerah di beijing dengan total ada 15 arah mata angin. Nilai pada indikator rain menunjukkan nilai 0 untuk semua data sehingga informasi yang bisa digali tidak ada, Ini juga sama dengan indikator station yang menunjukkan kesamaan data pada semua indikator.\n",
        "\n",
        "Selain itu, pada tabel sebelumya, arah mata angin berubah-ubah di tiap jamnya."
      ],
      "metadata": {
        "id": "UDHOD9eb786k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in pc_polution_final_df.columns:\n",
        "    if col not in ['station', 'datetime', 'wd', 'RAIN']:\n",
        "      group_data = pc_polution_final_df.groupby(by=['wd'])[col].mean()\n",
        "      max_station = group_data.idxmax()\n",
        "      max_value = group_data.max()\n",
        "      print(f\"Arah Mata Angin Dengan Nilai {col} Tertinggi : {max_station}, Nilai: {max_value}\")\n",
        "\n",
        "      min_station = group_data.idxmin()\n",
        "      min_value = group_data.min()\n",
        "      print(f\"Kota Dengan Nilai {col} Terendah : {min_station}, Nilai: {min_value}\\n\")"
      ],
      "metadata": {
        "id": "spSeIEt28aSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat pada hasil perulangan diatas, didapatkan bahwa arah mata angin NW mendapatkan nilai terendah pada indikator TEMP dan DEWP, lalu indikator tertinggi pada WSPM. Lalu ada juga arah mata angin SSE yang menjadi terendah di PRES dan tertinggi di DEWP."
      ],
      "metadata": {
        "id": "1wqw5Qau-OFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ü•® **Korelasi Antar Parameter**"
      ],
      "metadata": {
        "id": "9LSirkD0-vFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.drop(columns=['RAIN'], inplace=True)\n",
        "check_correlation(final_df)"
      ],
      "metadata": {
        "id": "2H3w6F5l-4OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secara keseluruhan, parameter polusi saling berkaita kuat antara satu dengan yang lainnya, sedangkan parameter pemicu polusi tidak demikian. Beberapa hubungan antar parameter seperti indikator PM2.5 dan PM10 berhubungan sangat lemah pada indikator titik embun (DEWP), lalu indikator SO2, NO2, dan CO berhubungan lemah juga dengan indikator tekanan angin (PRES). Namum indikator O3 berhubungan cukup kuat pada 2 indikator suhu (TEMP) dan kecepatan angin (WSPM).\n",
        "\n",
        "Jika melihat pada bagian parameter pemicu polusi, hanya indikator temperatur (TEMP) berhubungan kuat dengan indikator lainnya yakni indikator titik embun (DEWP)."
      ],
      "metadata": {
        "id": "gW7isaUZ_sie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ü•® **Geospatial Analysis**"
      ],
      "metadata": {
        "id": "KIJvmKEzONG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_inputs(year, month, day, hour):\n",
        "    \"\"\"\n",
        "    Validates the user input filters for year, month, day, and hour.\n",
        "\n",
        "    This function checks that the necessary filters are specified for each level\n",
        "    of time filtering. The user must provide filters in a logical sequence:\n",
        "    - Year is required if Month or Day are specified.\n",
        "    - Month is required if Day is specified.\n",
        "    - Day is required if Hour is specified.\n",
        "\n",
        "    Parameters:\n",
        "    - year (int or None): The selected year for filtering.\n",
        "    - month (int or None): The selected month for filtering.\n",
        "    - day (int or None): The selected day for filtering.\n",
        "    - hour (int or None): The selected hour for filtering.\n",
        "\n",
        "    Raises:\n",
        "    - ValueError: If the required filters are not specified in a valid order.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure that day is specified only if month and year are provided\n",
        "    if day is not None and (month is None or year is None):\n",
        "        raise ValueError(\"To filter by day, specify both year and month.\")\n",
        "\n",
        "    # Ensure that month is specified only if year is provided\n",
        "    if month is not None and year is None:\n",
        "        raise ValueError(\"To filter by month, specify the year.\")\n",
        "\n",
        "    # Ensure that hour is specified only if day, month, and year are provided\n",
        "    if hour is not None and (day is None or month is None or year is None):\n",
        "        raise ValueError(\"To filter by hour, specify year, month, and day.\")"
      ],
      "metadata": {
        "id": "8R4QWPgRxHWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define Stations Coordinates\n",
        "# Station coordinates get from outsources like google maps and google earth.\n",
        "# There is still have some issue that coordinat maybe wrong.\n",
        "stations_coords = {\n",
        "    \"Aotizhongxin\": (41.731242, 123.456778),\n",
        "    \"Changping\": (40.221, 116.2312),\n",
        "    \"Dingling\": (40.28998423518348, 116.2393424781757),\n",
        "    \"Dongsi\": (40.10208908941478, 116.31657335910373),\n",
        "    \"Guanyuan\": (39.94113871141321, 116.3610710753842),\n",
        "    \"Gucheng\": (39.91270053243136, 116.1868698799306),\n",
        "    \"Huairou\": (43.06043347888646, 117.46726428196578),\n",
        "    \"Nongzhanguan\": (39.93978579546827, 116.46859787734736),\n",
        "    \"Shunyi\": (40.151287025024715, 116.69280368021326),\n",
        "    \"Tiantan\": (39.88189413732897, 116.42047003643812),\n",
        "    \"Wanliu\": (39.99843210685499, 116.25774299569612),\n",
        "    \"Wanshouxigong\": (39.90816416629832, 116.26439549963654)\n",
        "}\n",
        "\n",
        "# Step 2: Make Visualization that According to Spesific Date Location\n",
        "# This step purpose to make filter based on year, month, day, and hour with\n",
        "# level of filter is Year -> Month -> Day -> Hour.\n",
        "selected_year = 2016\n",
        "selected_month = 11\n",
        "selected_day = None\n",
        "selected_hour = None\n",
        "\n",
        "# Step 3: Define a Function that Validate the Users Input\n",
        "# This step crucial to make rank filter to make spesific location\n",
        "# visualization later.\n",
        "validate_inputs(selected_year, selected_month, selected_day, selected_hour)\n",
        "\n",
        "# Step 4: Prepare DataFrame with Coordinates\n",
        "spatial_df = final_df.copy()\n",
        "spatial_df[\"latitude\"] = spatial_df[\"station\"].map(lambda x: stations_coords[x][0])\n",
        "spatial_df[\"longitude\"] = spatial_df[\"station\"].map(lambda x: stations_coords[x][1])\n",
        "spatial_df[\"datetime\"] = pd.to_datetime(spatial_df[\"datetime\"])\n",
        "\n",
        "# Step 5: Filter the Data According the User Input\n",
        "# At this section, users input are the variable that not None.\n",
        "# The filter work is based on rank filter first by year until hour.\n",
        "filtered_df = spatial_df.copy()\n",
        "if selected_year is not None:\n",
        "    filtered_df = filtered_df[filtered_df[\"datetime\"].dt.year == selected_year]\n",
        "if selected_month is not None:\n",
        "    filtered_df = filtered_df[filtered_df[\"datetime\"].dt.month == selected_month]\n",
        "if selected_day is not None:\n",
        "    filtered_df = filtered_df[filtered_df[\"datetime\"].dt.day == selected_day]\n",
        "if selected_hour is not None:\n",
        "    filtered_df = filtered_df[filtered_df[\"datetime\"].dt.hour == selected_hour]\n",
        "\n",
        "# Step 6: Determine Grouping and Hover Columns for Visualization\n",
        "# This step is crucial to make a new dataframe that spesific for\n",
        "# visualization later, so the data is small column that include\n",
        "# station name, latitude, longitude, and rank filter selection.\n",
        "group_cols = [\"station\", \"latitude\", \"longitude\"]\n",
        "if selected_hour is not None:\n",
        "    filtered_df.loc[:, \"hour\"] = filtered_df[\"datetime\"].dt.hour\n",
        "    group_cols = [\"hour\"] + group_cols\n",
        "    hover_cols = [\"hour\", \"PM2.5\"]\n",
        "elif selected_day is not None:\n",
        "    filtered_df.loc[:, \"day\"] = filtered_df[\"datetime\"].dt.day\n",
        "    group_cols = [\"day\"] + group_cols\n",
        "    hover_cols = [\"day\", \"PM2.5\"]\n",
        "elif selected_month is not None:\n",
        "    filtered_df.loc[:, \"month\"] = filtered_df[\"datetime\"].dt.month\n",
        "    group_cols = [\"month\"] + group_cols\n",
        "    hover_cols = [\"month\", \"PM2.5\"]\n",
        "elif selected_year is not None:\n",
        "    filtered_df.loc[:, \"year\"] = filtered_df[\"datetime\"].dt.year\n",
        "    group_cols = [\"year\"] + group_cols\n",
        "    hover_cols = [\"year\", \"PM2.5\"]\n",
        "\n",
        "# Step 7: Group and Aggregate Variabel PM2.5\n",
        "# This step will use example of 1 variabel for fast and less computation\n",
        "# visualization. The groupping dataframe will accordding to level rank\n",
        "# that input by users.\n",
        "filtered_df = (\n",
        "    filtered_df.groupby(group_cols)\n",
        "    .agg({\"PM2.5\": \"mean\"})\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Step 8: Plot the Visualization on the Map Using Ploty\n",
        "fig = px.scatter_mapbox(\n",
        "    filtered_df,\n",
        "    lat=\"latitude\",\n",
        "    lon=\"longitude\",\n",
        "    color=\"PM2.5\",\n",
        "    size=\"PM2.5\",\n",
        "    hover_name=\"station\",\n",
        "    hover_data=hover_cols,\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    title=\"Time-Series Geospatial Data\",\n",
        "    zoom=5,\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ZNPUer7IOM6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat pada gambar peta diatas, ada beberapa titik lokasi yang memiliki perbedaan warna titik pada satu daerah dengan daerah yang lainnya. Perbedaan warna ini dipengaruhi oleh tingkat konsentrasi parameter polusi PM2.5 nya pada suatu daerah. Semakin pekat maka semakin tinggi, sedangkan sebaliknya semakin cerah/terang maka semakin rendah."
      ],
      "metadata": {
        "id": "8X_1C0sj0SY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚è© **Insight EDA (Eksploratory Data Analysis)**\n",
        "\n",
        "Secara keseluruhan dalam proses ekploratory dibagi menjadi 2 tahap yakni tahapan persiapan dan eksplorasi. Untuk tahap persiapan dilakukan persiapan data dengan menggabungkan semua dataframe daerah menjadi satu untuk memudahkan proses analisis. Lalu, membagi kembali menjadi 2 dataframe dengan 2 kategori berbeda yakni : Data Parameter Polusi dan Data Parameter Pemicu Polusi.\n",
        "\n",
        "- Pada hasil eksplorasi data parameter polusi didapatkan banyak hal terkait dengan daerah Dongsi yang menjadi pemilik indikator tertinggi yakni pada PM2.5, SO2, dan CO, sedangkan daerah Dingling dengan paling banyak indikator polusi terendah yakni PM2.5, PM10, NO2, dan CO.\n",
        "\n",
        "- Pada eksplorasi parameter polusi juga didapatkan bahwa data dapat diubah dengan ukuran dalam satuan jam, hari, minggu, bulan, dan tahun. Dengan ini menandakan bisa membuat visualisasi yang mendetail dan rinci dalam implementasi nanti.\n",
        "\n",
        "- Dilanjutkan dengan proses eksplorasi parameter pemicu polusi, didapatkan bahwa indikator station dan curah hujan (RAIN) tidak memiliki perlakuan yang berbeda untuk data jadi dapat dihilangkan.\n",
        "\n",
        "- Selain itu didapatkan juga arah mata angin (wd) yang memiliki nilai tertinggi dan terendah terdapat pada arah mata angin SSE dan NW.\n",
        "\n",
        "- Selanjutnya adalah melihat korelasi data setelah digabungkan dengan menghilangkan indikator curah hujan (RAIN) yang tidak berdampak signifikan. Didapatkan bahwa parameter polusi saling berhubungan antara satu sama lain, sedangkan parameter pemicu polusi yang berhubungan dengan baik hanya indikator temperatur (TEMP) dengan indikator titik embun (DEWP).\n",
        "\n",
        "- Yang terakhir adalah analisis tambahan pada analisis geospasial menggunakan ploty. Pada analisis ini bertujuan untuk melihat perbandingan nilai variabel PM2.5 pada masing-masing daerah jika dipetakan dalam ruang spasial. Untuk variabel bisa digunakan yang lain seperti PM10, dan lainnya agar visualisasi menjadi lebih mendetail.\n",
        "\n"
      ],
      "metadata": {
        "id": "evc3i673CMs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí¨ **Data Visualization dan Explanatory**"
      ],
      "metadata": {
        "id": "WF1zXlX6F3D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahapan Data Visualization dan Explanatory ini akan menjelaskan insight yang ditemukan pada data untuk menjawab pertanyaan bisnis yang sudah diajukan sebelumnya, untuk pertanyaan bisnisnya sebagai berikut:\n",
        "\n",
        "1. Bagaimana tren polusi udara pada masing-masing daerah di Beijing?\n",
        "2. Kapan waktu udara terparah terjadi di masing-masing daerah di Beijing?\n",
        "3. Apakah ada korelasi antar parameter pemicu (TEMP, DEWP, RAIN, Wd, WSPM) dengan parameter polusi (PM2.5, PM10, SO2, NO2, CO, O3)? Jika ada, maka bagaimana korelasinya?\n",
        "\n",
        "Ketiga pertanyaan ini akan dijawab dalam 3 segmen berbeda dengan segmen terakhir adalah kesimpulan akhir dari Data Visualization dan Explanatory.\n",
        "\n",
        "Untuk dataframe yang digunakan adalah `final_df`, `p_polution_final_df`, dan `pc_polution_final_df`."
      ],
      "metadata": {
        "id": "HydsiLjO01yQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pertanyaan 1: Bagaimana tren polusi udara pada masing-masing daerah di Beijing?**"
      ],
      "metadata": {
        "id": "fX3aS2gw1VFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_data(df, visualization_type, component_date={'year': None, 'month': None, 'days': None}):\n",
        "    \"\"\"\n",
        "    Aggregates the given DataFrame based on the specified visualization type (Annual, Monthly, Daily, or Hourly).\n",
        "\n",
        "    This function computes aggregated statistics (mean) on air quality data, grouped by time-based components\n",
        "    such as year, month, day, or hour. The function can be customized to aggregate data for a specific year,\n",
        "    month, or day if necessary.\n",
        "\n",
        "    Parameters:\n",
        "    - df : pandas.DataFrame\n",
        "        The input DataFrame containing air quality data. Must include a 'datetime' column and pollutant data columns\n",
        "        (e.g., 'PM2.5', 'PM10').\n",
        "\n",
        "    - visualization_type : str\n",
        "        The type of aggregation to perform. Accepted values are:\n",
        "        - 'Annual' : Aggregates data by year.\n",
        "        - 'Monthly' : Aggregates data by month, optionally for a specific year.\n",
        "        - 'Daily' : Aggregates data by day, optionally for a specific month and year.\n",
        "        - 'Hours' : Aggregates data by hour, optionally for a specific day, month, and year.\n",
        "\n",
        "    - component_date : dict, optional (default is {'year': None, 'month': None, 'days': None})\n",
        "        A dictionary specifying the specific year, month, and day for monthly, daily, and hourly aggregations.\n",
        "        The dictionary may include:\n",
        "        - 'year' : The specific year for monthly, daily, or hourly aggregation.\n",
        "        - 'month' : The specific month for daily or hourly aggregation.\n",
        "        - 'days' : The specific day for hourly aggregation.\n",
        "        If not provided, the function will aggregate based on all available data.\n",
        "\n",
        "    Returns:\n",
        "    - agg_df : pandas.DataFrame\n",
        "        The aggregated DataFrame based on the chosen visualization type. The data is grouped by relevant time units\n",
        "        (e.g., year, month, day, or hour), and the mean of each pollutant is computed for each group.\n",
        "\n",
        "    - id_vars : list\n",
        "        A list of columns used as identifier variables for the aggregation. These columns depend on the selected\n",
        "        aggregation type (e.g., ['station', 'year'] for annual aggregation).\n",
        "\n",
        "    - labels : list\n",
        "        A list containing the name of the time unit used in the aggregation, along with its descriptive label\n",
        "        (e.g., ['year', 'Year']).\n",
        "\n",
        "    Raises:\n",
        "    - ValueError : If 'datetime' column is missing in the DataFrame.\n",
        "    - ValueError : If an invalid 'visualization_type' is provided.\n",
        "    - ValueError : If required values for year, month, or day are missing when performing monthly, daily, or hourly aggregation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Raise Value Error if datetime Column not present.\n",
        "    if 'datetime' not in df.columns:\n",
        "      raise ValueError(\"'datetime' column is missing in the DataFrame.\")\n",
        "\n",
        "    # Detect Year, Month, Day, and Hour\n",
        "    df.loc[:, 'year'] = df['datetime'].dt.year\n",
        "    df.loc[:, 'month'] = df['datetime'].dt.month\n",
        "    df.loc[:, 'day'] = df['datetime'].dt.day\n",
        "    df.loc[:, 'hour'] = df['datetime'].dt.hour\n",
        "\n",
        "    # Detect Unique Component Datetime\n",
        "    unique_years = df['year'].nunique(),\n",
        "    unique_months = df['month'].nunique(),\n",
        "    unique_days = df['day'].nunique(),\n",
        "    unique_hours = df['hour'].nunique()\n",
        "\n",
        "    # Initialize variables\n",
        "    agg_df = None\n",
        "    id_vars = []\n",
        "    labels = []\n",
        "\n",
        "    # Use match-case for aggregation\n",
        "    match visualization_type:\n",
        "        case 'Annual':\n",
        "            agg_df = df.groupby(['station', 'year']).mean().reset_index()\n",
        "            id_vars = ['station', 'year']\n",
        "            labels = ['year', 'Year']\n",
        "\n",
        "        # Case for Single Year and More than One Year\n",
        "        case 'Monthly' if df['year'].nunique() == 1:\n",
        "            agg_df = df.groupby(['station', 'month']).mean().reset_index()\n",
        "            id_vars = ['station', 'month']\n",
        "            labels = ['month', 'Month']\n",
        "        case 'Monthly':\n",
        "            if component_date['year'] is None:\n",
        "                raise ValueError(f\"Year is required for monthly aggregation.\")\n",
        "            selected_year = component_date['year']\n",
        "            agg_df = df[df['year'] == selected_year].groupby(['station', 'month']).mean().reset_index()\n",
        "            id_vars = ['station', 'month']\n",
        "            labels = ['month', 'Month']\n",
        "\n",
        "        # Case for Single Month and More than One Months\n",
        "        case 'Daily' if df['month'].nunique() == 1:\n",
        "            agg_df = df.groupby(['station', 'day']).mean().reset_index()\n",
        "            id_vars = ['station', 'day']\n",
        "            labels = ['day', 'Day']\n",
        "        case 'Daily':\n",
        "            if component_date['year'] is None or component_date['month'] is None:\n",
        "                raise ValueError(f\"Year and Month are required for daily aggregation.\")\n",
        "            selected_year = component_date['year']\n",
        "            selected_month = component_date['month']\n",
        "            agg_df = df[(df['year'] == selected_year) & (df['month'] == selected_month)].groupby(['station', 'day']).mean().reset_index()\n",
        "            id_vars = ['station', 'day']\n",
        "            labels = ['day', 'Day']\n",
        "\n",
        "        # Case for Single Day and More than One Days\n",
        "        case 'Hours' if df['day'].nunique() == 1:\n",
        "            agg_df = df.groupby(['station', 'hour']).mean().reset_index()\n",
        "            id_vars = ['station', 'hour']\n",
        "            labels = ['hour', 'Hour']\n",
        "        case 'Hours':\n",
        "            if component_date['year'] is None or component_date['month'] is None or component_date['days'] is None:\n",
        "                raise ValueError(f\"Year, Month, and Day are required for hourly aggregation.\")\n",
        "            selected_year = component_date['year']\n",
        "            selected_month = component_date['month']\n",
        "            selected_day = component_date['days']\n",
        "            agg_df = df[(df['year'] == selected_year) & (df['month'] == selected_month) & (df['day'] == selected_day)].groupby(['station', 'hour']).mean().reset_index()\n",
        "            id_vars = ['station', 'hour']\n",
        "            labels = ['hour', 'Hour']\n",
        "\n",
        "        # Case for if all case above not match\n",
        "        case _:\n",
        "            raise ValueError(\"Invalid visualization type. Choose from 'Annual', 'Monthly', 'Daily', 'Hours'.\")\n",
        "\n",
        "    return agg_df, id_vars, labels\n",
        "\n",
        "\n",
        "def ploty_line_visualization(data, id_vars, labels, parameters, title_template=\"Average Concentration Data by Station and {}\"):\n",
        "    \"\"\"\n",
        "    Creates a Plotly line chart to visualize air quality data by station and a specified time unit (e.g., year, month, hour).\n",
        "\n",
        "    Parameters:\n",
        "    - data : pandas.DataFrame\n",
        "        The input data containing station names, time variables, and pollutant concentrations.\n",
        "        The DataFrame must include columns corresponding to the time unit(s), pollutant concentrations, and station identifiers.\n",
        "\n",
        "    - id_vars : list\n",
        "        The identifier variables used to group the data (e.g., ['station', 'year']).\n",
        "\n",
        "    - labels : list\n",
        "        A list containing the name of the time unit column and its descriptive label (e.g., ['year', 'Year']).\n",
        "\n",
        "    - parameters : list\n",
        "        A list of pollutant columns to include in the visualization (e.g., ['PM2.5', 'PM10']).\n",
        "\n",
        "    - title_template : str, optional (default: \"Concentration Data by Station and {}\")\n",
        "        A string template for the chart title, with the placeholder `{}` being replaced by the descriptive label of the time unit.\n",
        "\n",
        "    Returns:\n",
        "    - plotly.graph_objects.Figure\n",
        "        A Plotly figure object representing the line chart, visualizing the pollutant concentrations by station over the specified time unit(s).\n",
        "    \"\"\"\n",
        "\n",
        "    # For Better Visualization will Using Long Format with Function melt()\n",
        "    df_melted = data.melt(\n",
        "        id_vars=id_vars,\n",
        "        value_vars=parameters,\n",
        "        var_name='Pollutant',\n",
        "        value_name='Concentration'\n",
        "    )\n",
        "\n",
        "    # Define Free Variabel to Set the Visualization Plot\n",
        "    if 'month' in df_melted.columns:\n",
        "        month_names = {\n",
        "            1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',\n",
        "            7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'\n",
        "        }\n",
        "        df_melted['month_name'] = df_melted['month'].map(month_names)\n",
        "        labels[0] = 'month_name'\n",
        "        tick_mode = 'array'\n",
        "        tick_val = list(month_names.values())\n",
        "        tick_text = tick_val\n",
        "        tick_angle = 0\n",
        "    elif 'hour' in df_melted.columns:\n",
        "        df_melted['AM_PM'] = df_melted['hour'].apply(lambda x: f\"{x % 12 or 12} {'AM' if x < 12 else 'PM'}\")\n",
        "        labels[0] = 'AM_PM'\n",
        "        tick_mode = 'array'\n",
        "        tick_val = df_melted['AM_PM'].unique()\n",
        "        tick_text = tick_val\n",
        "        tick_angle = 90\n",
        "    else:\n",
        "        tick_mode = 'linear'\n",
        "        tick_val = None\n",
        "        tick_text = None\n",
        "        tick_angle = None\n",
        "\n",
        "    fig = px.line(\n",
        "        df_melted,\n",
        "        x=f\"{labels[0]}\",\n",
        "        y='Concentration',\n",
        "        color='station',\n",
        "        labels={labels[0]: labels[1], 'Concentration': 'Concentration', 'station': 'Station'},\n",
        "        line_shape='linear'\n",
        "    )\n",
        "\n",
        "    # Set the Layout Visualization\n",
        "    fig.update_layout(\n",
        "        title=title_template.format(labels[1]),\n",
        "        legend_title='Station',\n",
        "        template='simple_white',\n",
        "        width=900,\n",
        "        height=450,\n",
        "        title_font=dict(size=16),\n",
        "        xaxis=dict(\n",
        "            title=\"\",\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12),\n",
        "            showgrid=False,\n",
        "            gridwidth=1,\n",
        "            gridcolor='white',\n",
        "            tickmode=tick_mode,\n",
        "            tickformat=\".0f\",\n",
        "            tickvals=tick_val,\n",
        "            ticktext=tick_text,\n",
        "            tickangle=tick_angle\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            title=\"\",\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12),\n",
        "            showgrid=True,\n",
        "            gridwidth=1,\n",
        "            gridcolor='lightgrey'\n",
        "        ),\n",
        "        legend=dict(\n",
        "            title='Station',\n",
        "            font=dict(size=12),\n",
        "        ),\n",
        "        hovermode='closest',\n",
        "        plot_bgcolor='white',\n",
        "        paper_bgcolor='ghostwhite'\n",
        "    )\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "lKyE2Hm_Nv1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pertanyaan mengenai tren polusi udara, maka dapat dijawab dengan menggunakan plot visualisasi garis dengan membandingkan antar daerah tergantung pada parameter polusinya."
      ],
      "metadata": {
        "id": "ZerhJtWV1ajo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_polution_final_df.head()"
      ],
      "metadata": {
        "id": "Zmh4wBtp16hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 : Make Filter Data\n",
        "start_date = '2013-03-01 00:00:00'\n",
        "end_date = '2017-03-31 23:00:00'\n",
        "\n",
        "filter_df = p_polution_final_df.loc[(p_polution_final_df['datetime'] >= start_date) & (p_polution_final_df['datetime'] <= end_date)].copy()"
      ],
      "metadata": {
        "id": "e_qS7sHmlTgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define Variabel and Make Aggregate Data in Years, Month, Day, and Hours.\n",
        "choose_visualization = 'Annual'\n",
        "choose_parameter = ['PM2.5']\n",
        "agg_data, id_vars, labels = aggregate_data(filter_df, choose_visualization)\n",
        "\n",
        "# Step 3: Do Visualization\n",
        "agg_data.drop(columns=['month', 'day', 'hour'], inplace=True)\n",
        "fig = ploty_line_visualization(agg_data, id_vars, labels, choose_parameter)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "neQoU6oU2dxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate and Visualize Monthly in A Year\n",
        "\n",
        "choose_visualization = 'Monthly'\n",
        "choose_parameter = ['PM2.5']\n",
        "\n",
        "agg_data, id_vars, labels = aggregate_data(filter_df, choose_visualization, component_date= {'year': 2013})\n",
        "agg_data.drop(columns=['year', 'day', 'hour'], inplace=True)\n",
        "\n",
        "fig = ploty_line_visualization(agg_data, id_vars, labels, choose_parameter)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "wkb2qCIFRy8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate and Visualize Daily in a Month\n",
        "\n",
        "choose_visualization = 'Daily'\n",
        "choose_parameter = ['PM2.5']\n",
        "\n",
        "agg_data, id_vars, labels = aggregate_data(filter_df, choose_visualization, component_date={'year': 2013, 'month': 3})\n",
        "agg_data.drop(columns=['month', 'year', 'hour'], inplace=True)\n",
        "\n",
        "fig = ploty_line_visualization(agg_data, id_vars, labels, choose_parameter)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "iKvLfZLQSGmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate and Visualize Hourly in a Day\n",
        "\n",
        "choose_visualization = 'Hours'\n",
        "choose_parameter = ['PM2.5']\n",
        "\n",
        "agg_data, id_vars, labels = aggregate_data(filter_df, choose_visualization, component_date={'year': 2013, 'month': 3, 'days': 1})\n",
        "agg_data.drop(columns=['month', 'day', 'year'], inplace=True)\n",
        "\n",
        "fig = ploty_line_visualization(agg_data, id_vars, labels, choose_parameter)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QBjqf4-sSgG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîÜ **Explanatory Pertanyaan Pertama**\n",
        "\n",
        "Dalam segmen pertanyaan pertama ini mengenai tren pada data Air Quality - Parameter Polusi dilakukan dengan analisis berdasarkan pembagian waktunya. Berikut adalah pembagian analisisnya:\n",
        "1. Pembagian Berdasarkan Tahun Keseluruhan\n",
        "2. Pembagian Berdasarkan Bulan per Tahunnya\n",
        "3. Pembagian Berdasarkan Tanggal per Bulannya\n",
        "4. Pembagian Berdasarkan Jam per Harinya\n",
        "\n",
        "---\n",
        "\n",
        "##### **1. Pembagian Berdasarkan Tahun Keseluruhan**\n",
        "Dapat dilihat pada visualisasi data mengenai rata-rata dalam [hitungan tahun keseluruhan](https://colab.research.google.com/drive/1rCfERccLm7CMNkF_BvSk7RpwDR8Q-hCn#scrollTo=neQoU6oU2dxJ&line=1&uniqifier=1). Terdapat peningkatan nilai rata-rata parameter polusi yakni PM2.5 di seluruh derah beijing. Dapat dilihat juga pada grafik, 3 daerah tertinggi nilai parameter PM2.5 di tahun 2013 adalah Wanliu, Dongsi, dan Nongzhanguan. Lalu di 2017 terdapat perubahan yakni yang tertinggi adalah daerah Wanshouxigong, Dongsi, dan Nongzhanguan.  \n",
        "\n",
        "Lalu, selain daerah dengan tingkat PM2.5 tertinggi adapun daerah yang menjadi 3 terendah yakni daerah Dingling, Huairio, dan Changping. Terutama daerah Dingling menjadi daerah satu-satunya yang konsisten menjadi yang terendah untuk tingkat indikator PM2.5.\n",
        "\n",
        "Untuk analisis lebih lanjut bisa mencoba melihat parameter polusi lainnya seperti PM10, SO2, NO2, CO, dan O3.\n",
        "\n",
        "##### **2. Pembagian Berdasarkan Bulan per Tahunnya**\n",
        "\n",
        "Dapat dilihat pada visualisasi data mengenai rata-rata parameter polusi berdasarkan dari [hitungan bulan tiap tahun](https://colab.research.google.com/drive/1rCfERccLm7CMNkF_BvSk7RpwDR8Q-hCn#scrollTo=wkb2qCIFRy8v&line=1&uniqifier=1). Pada tahun 2013, Wanliu termasuk daerah dengan rata-rata tertinggi untuk indeks PM2.5-nya, terutama puncaknya ada pada bulan Juni sebesar 109. Selain itu untuk daerah yang memiliki rata-rata tahunan terendah untuk indikator PM2.5 adalah Dingling, sempat mengalami kenaikan di bulan Juni akan tetapi untuk bulan-bulan selanjutnya konsisten menjadi terendah. Jika menilik ke tahun terbaru yakni tahun 2016 menjadi tahun dengan indeks PM2.5 terendah dikeseluruhan data, hal ini dapat dilihat pada data visualisasi bulan tiap tahunnya, terjadi penurunan drastis untuk semua daerah di bulan january ke bulan february, penurunan terjadi terhadap semua daerah.\n",
        "\n",
        "##### **3. Pembagian Berdasarkan Tanggal per Bulannya**\n",
        "Dapat dilihat pada visualisasi data mengenai rata-rata parameter polusi berdasarkan dari [hitungan hari tiap bulan](https://colab.research.google.com/drive/1rCfERccLm7CMNkF_BvSk7RpwDR8Q-hCn#scrollTo=iKvLfZLQSGmQ&line=6&uniqifier=1). Pada tahun 2016, di bulan January ada penurunan drastis indeks PM2.5 selama 2 hari, yakni dari tanggal 1-4 January. Tentu daerah yang menjadi titik terendah adalah Dingling. Hal yang menarik juga, jika melihat data bulan february, dalam 4 hari yakni tanggal 13-16 February semua daerah konsisten rendah (linear) tidak ada peningkatan drastis atau lonjakan konsentrasi parameter PM2.5.\n",
        "\n",
        "##### **4. PPembagian Berdasarkan Jam per Harinya**\n",
        "Dapat dilihat pada visualisasi data mengenai rata-rata parameter polusi berdasarkan dari [hitungan jam perharinya](https://colab.research.google.com/drive/1rCfERccLm7CMNkF_BvSk7RpwDR8Q-hCn#scrollTo=QBjqf4-sSgG7&line=6&uniqifier=1). Jika kita coba menilik pada data terendah di 2016, yakni dibulan Februari tanggal 13-16. Ditanggal 13, dimulai dari pukul 12AM-7AM, kecuali daerah Huario, daerah lainnya tidak mengalami lonjakan yang signifikan, lalu dari pukul 8AM-11PM, terjadi lonjakan konsentrasi paramater PM2.5, akan tetapi daerah Dingling tetap stabil tidak ada lonjakan yang tinggi. Di hari selanjutnya, semua daerah terjadi lonjakan yang signifikan untuk konsentrasi parameter PM2.5 nya, trend lonjakan paling tinggi adalah daerah Guncheng dengan titik tertingginya pukul 8 PM di 157. Di hari berikutnya kembali, pada pukul 1AM-7AM terjadi trend penurunan konsentrasi PM2.5, lalu pukul 12PM-11PM terjadi trend peningkatan konsentrasi parameter PM2.5. Lalu di hari terakhir yakni tanggal 16 dimulai pukul 12AM-6AM terjadi trend penurunan konsentrasi PM2.5 sedangkan pukul 7AM-11PM terjadi trend kenaikan konsentrasi PM2.5"
      ],
      "metadata": {
        "id": "5DDcDFpzatwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pertanyaan 2: Kapan waktu udara terparah terjadi di masing-masing daerah di Beijing?**"
      ],
      "metadata": {
        "id": "NZ71IDuqS9dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ploty_bar_visualization(data, id_vars, labels, parameters, title_template=\"Average Concentration Data by Station and {}\"):\n",
        "    \"\"\"\n",
        "    Visualizes pollutant concentration data in a stacked bar plot, with dynamic hover information\n",
        "    based on the type of time-related data available (month, year, day, or AM/PM).\n",
        "\n",
        "    Parameters:\n",
        "    - data : pandas.DataFrame\n",
        "        The input dataframe containing the data to be visualized.\n",
        "\n",
        "    - id_vars : list of str\n",
        "        The columns in the dataframe that will be used for grouping the x-axis (e.g., station name).\n",
        "\n",
        "    - labels : list of str\n",
        "        A list where the first item is the column used for coloring the bars (e.g., 'year')\n",
        "        and the second item is the label for that column in the plot.\n",
        "\n",
        "    - parameters : list of str\n",
        "        A list containing the names of the pollutant parameters (e.g., ['PM2.5']) to be plotted.\n",
        "\n",
        "    - title_template : str, optional, default=\"Average Concentration Data by Station and {}\"\n",
        "        The template for the plot's title. The `{}` placeholder will be replaced with the second\n",
        "        item in the `labels` list.\n",
        "\n",
        "    Returns:\n",
        "    - plotly.graph_objects.Figure\n",
        "        The Plotly figure object containing the bar plot visualization.\n",
        "    \"\"\"\n",
        "\n",
        "    # Customize Month Column and Hover\n",
        "    if 'month' in data.columns:\n",
        "      month_names = {\n",
        "          1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',\n",
        "          7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'\n",
        "      }\n",
        "      data['Month'] = data['month'].map(month_names)\n",
        "      hover_data={'Month': True, 'month':False}\n",
        "      custom_value = 'Month'\n",
        "      hover_temp = \"Month: %{customdata[0]}<br>\"\n",
        "\n",
        "    # Customize Year Column and Hover\n",
        "    elif 'year' in data.columns:\n",
        "      hover_data=None\n",
        "      custom_value = 'year'\n",
        "      hover_temp= \"Year: %{customdata[0]}<br>\"\n",
        "\n",
        "    # Customize Day Column and Hover\n",
        "    elif 'day' in data.columns:\n",
        "      hover_data=None\n",
        "      custom_value = 'day'\n",
        "      hover_temp= \"Day: %{customdata[0]}<br>\"\n",
        "\n",
        "    # Customize Clocl Column and Hover\n",
        "    else:\n",
        "      data['AM_PM'] = data['hour'].apply(lambda x: f\"{x % 12 or 12} {'AM' if x < 12 else 'PM'}\")\n",
        "      hover_data=None\n",
        "      custom_value = 'AM_PM'\n",
        "      hover_temp= \"Hour: %{customdata[0]}<br>\"\n",
        "\n",
        "    # Do Sorting Data by Parameter\n",
        "    data = data.sort_values(by=parameters[0])\n",
        "\n",
        "    # Wrap All Variabel Visualization\n",
        "    fig = px.bar(\n",
        "        data,\n",
        "        x=id_vars[0],\n",
        "        y=parameters,\n",
        "        color=labels[0],\n",
        "        barmode='stack',\n",
        "        hover_data=hover_data,\n",
        "    )\n",
        "\n",
        "    # Customize Hover\n",
        "    fig.update_traces(customdata=data[[custom_value]].values)\n",
        "    fig.update_traces(\n",
        "        hovertemplate=\"<b>%{x}</b><br>\"\n",
        "        + \"Concentration: %{y:.2f}<br>\"\n",
        "        + hover_temp\n",
        "    )\n",
        "\n",
        "    # Set the Layout Visualization\n",
        "    fig.update_layout(\n",
        "        title=title_template.format(labels[1]),\n",
        "        legend_title='Station',\n",
        "        template='simple_white',\n",
        "        width=900,\n",
        "        height=450,\n",
        "        title_font=dict(size=16),\n",
        "        xaxis=dict(\n",
        "            title=\"\",\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12),\n",
        "            showgrid=False,\n",
        "            gridwidth=1,\n",
        "            gridcolor='white',\n",
        "            tickformat=\".0f\",\n",
        "            tickangle=45\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            title=\"\",\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12),\n",
        "            showgrid=True,\n",
        "            gridwidth=1,\n",
        "            gridcolor='lightgrey'\n",
        "        ),\n",
        "        legend=dict(\n",
        "            title='Station',\n",
        "            font=dict(size=12),\n",
        "        ),\n",
        "        hovermode='closest',\n",
        "        plot_bgcolor='white',\n",
        "        paper_bgcolor='ghostwhite'\n",
        "    )\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "vL1XAjXsEFom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 : Make filter data\n",
        "start_date = '2013-03-01 00:00:00'\n",
        "end_date = '2017-03-31 23:00:00'\n",
        "\n",
        "filter_df = p_polution_final_df.loc[(p_polution_final_df['datetime'] >= start_date) & (p_polution_final_df['datetime'] <= end_date)].copy()"
      ],
      "metadata": {
        "id": "NgioSzGVTC99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define Variabel and Make Aggregate Data in Years, Month, Day, and Hours.\n",
        "# Aggregate in Years Total\n",
        "choose_visualization = 'Annual'\n",
        "choose_parameter = ['PM2.5']\n",
        "agg_data, id_vars , labels = aggregate_data(filter_df, choose_visualization)\n",
        "\n",
        "# Step 3: Do Visualization\n",
        "agg_data.drop(columns=['month', 'day', 'hour'], inplace=True)\n",
        "fig = ploty_bar_visualization(agg_data, id_vars, labels, choose_parameter)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "L0oecyKwCX8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate and Visualize Monthly in A Year\n",
        "choose_visualization = 'Monthly'\n",
        "choose_parameter = ['PM2.5']\n",
        "\n",
        "agg_data, id_vars, labels = aggregate_data(filter_df, choose_visualization, component_date= {'year': 2013})\n",
        "agg_data.drop(columns=['year', 'day', 'hour'], inplace=True)\n",
        "\n",
        "fig = ploty_bar_visualization(agg_data, id_vars, labels, choose_parameter)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "5HqAn7A_hD8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate and Visualize Daily in a Month\n",
        "choose_visualization = 'Daily'\n",
        "choose_parameter = ['PM2.5']\n",
        "\n",
        "agg_data, id_vars, labels = aggregate_data(filter_df, choose_visualization, component_date={'year': 2013, 'month': 3})\n",
        "agg_data.drop(columns=['month', 'year', 'hour'], inplace=True)\n",
        "\n",
        "fig = ploty_bar_visualization(agg_data, id_vars, labels, choose_parameter)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "qd8tj5q95k95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate and Visualize Hourly in a Day\n",
        "choose_visualization = 'Hours'\n",
        "choose_parameter = ['PM2.5']\n",
        "\n",
        "agg_data, id_vars, labels = aggregate_data(filter_df, choose_visualization, component_date={'year': 2013, 'month': 3, 'days': 1})\n",
        "agg_data.drop(columns=['month', 'day', 'year'], inplace=True)\n",
        "\n",
        "fig = ploty_bar_visualization(agg_data, id_vars, labels, choose_parameter)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "MPLsJzGh7J0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîÜ **Explanatory Pertanyaan Kedua**\n",
        "\n",
        "Dapat dilihat pada visualisasi dengan bentuk stack-bar diatas, visualisasi dilakukan dengan membagi dalam beberapa sub-bagian, pembagiannya sebagai berikut:\n",
        "1. Pembagian Data dalam Hitungan Tahun.\n",
        "2. Pembagian Data dalam Hitungan Bulan per Tahunnya.\n",
        "3. Pembagian Data dalam Hitungan Hari per Bulannya.\n",
        "4. Pembagian Data dalam Hitungan Jam per Harinya.\n",
        "\n",
        "> *Untuk membuat penjelasan lebih khusus maka akan digunakan tanggal 1 Maret 2013 sebagai contoh pembahasan.*\n",
        "\n",
        "---\n",
        "\n",
        "##### **1. Pembagian Data dalam Hitungan Tahun**\n",
        "Dalam visualisasi pembagian data dalam [hitungan tahun](https://colab.research.google.com/drive/1rCfERccLm7CMNkF_BvSk7RpwDR8Q-hCn#scrollTo=L0oecyKwCX8m&line=1&uniqifier=1). Dapat dilihat pada masing-masing visualisasi yang dikelompokkan berdasarkan station memiliki nilai tertinggi parameter PM2.5 yang berbeda-beda, Salah satu contohnya pada daerah Wanliu tahun 2014 merupakan nilai tertinggi indeks polusi PM2.5-nya, sedangkan pada daerah Dongsi dan Wanshouxgong berada di tahun 2017. Untuk Indeks terendah menariknya semua daerah berada di tahun 2016. Lalu untuk indeks tertinggi yang paling banyak terjadi pada tahun 2014.    \n",
        "\n",
        "##### **2. Pembagian Data dalam Hitungan Bulan per Tahunnya**\n",
        "Dalam visualisasi pembagian data dalam [hitungan bulan per tahunnya](https://colab.research.google.com/drive/1rCfERccLm7CMNkF_BvSk7RpwDR8Q-hCn#scrollTo=5HqAn7A_hD8v&line=1&uniqifier=1). Dapat dilihat pada masing-masing pembagian data dikelompokkan berdasarkan station dan tahunnya, yakni pada 2013. Pada tahun ini, bulan Juli merupakan bulan dengan index tertinggi pada daerah Beijing yang paling banyak, lalu disusul oleh maret dan mei. Untuk indeks terendah pada masing-masing kota bervariasi, akan tetapi khusus daerah Huairou satu-satunya memiliki indeks tertinggi di bulan Mei.\n",
        "\n",
        "##### **3. Pembagian Data dalam Hitungan Hari per Bulannya**\n",
        "Dalam visualisasi pembagian data dalam [hitungan hari per bulannya](https://colab.research.google.com/drive/1rCfERccLm7CMNkF_BvSk7RpwDR8Q-hCn#scrollTo=qd8tj5q95k95&line=8&uniqifier=1). Dapat dilihat pada masing-masing visualisasi pembagian data dikelompokkan berdasarkan station, tahun, dan bulannya di Tahun 2013. Pada bulan Maret ini jika coba dizoom untuk melihat nilai terkecilnya maka hari pertama (Day-1)adalah hari dengan nilai indeks terkecil dari semua daerah, dilanjutkan dengan indeks tertinggi dipegang oleh hari ke-7 dan ke-17, daerah Wanliu satu-satunya daerah dengan indeks PM2.5 tertinggi di hari ke-6.\n",
        "\n",
        "##### **4. Pembagian Data dalam Hitungan Jam per Harinya**\n",
        "Dalam visualisasi pembagian data dalam [hitungan jam per harinya](https://colab.research.google.com/drive/1rCfERccLm7CMNkF_BvSk7RpwDR8Q-hCn#scrollTo=MPLsJzGh7J0T&line=1&uniqifier=1). Dapat dilihat pada masing-masing visualisasi pembagian data dikelompokkan berdasarkan station, tahun, bulan, dan harinya di tahun 2023. Ini mendapatkan data indikator PM2.5 dalam kurun waktu 24 jam di hari bersangkutan, pada contoh adalah Day ke-1. Indeks tertinggi dominan berada pada pukul 11 PM, lalu indeks terkecil pada masing-masing daerah beragam. Daerah yang cukup menarik adalah Dingling sebagai daerah dengan total indeks hariannya paling kecil yakni kurang dari 200, sedangkan daerah Guanyuan bisa sampai > 250."
      ],
      "metadata": {
        "id": "d6grVOSd-Sqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pertanyaan 3: Apakah ada korelasi antar parameter pemicu (TEMP, DEWP, RAIN, Wd, WSPM) dengan parameter polusi (PM2.5, PM10, SO2, NO2, CO, O3)? Jika ada, maka bagaimana korelasinya?**"
      ],
      "metadata": {
        "id": "UEebGoVaE025"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_correlation(final_df)"
      ],
      "metadata": {
        "id": "MZ1HJzjmE7DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîÜ **Explanatory Pertanyaan Ketiga**\n",
        "\n",
        "Dalam melihat keterhubungan antara variabel dapat menggunakan matrix korelasi seperti visualisasi diatas, untuk memahami matrixnya dapat dibagi menjadi 2 pemahaman:\n",
        "1. Pemahaman korelasi variabel polusi dengan variabel pemicu polusi.\n",
        "2. Pemahaman korelasi antar variabel polusi atau pemicu polusi.\n",
        "\n",
        "Dengan 2 pemahaman ini akan dapat mengetahui mengenai keterhubungan parameter-parameter baik dalam lingkup sejenis atau antar lingkup. Berikut adalah pembahasan detailnya.\n",
        "\n",
        "---\n",
        "\n",
        "##### **1. Pemahaman Korelasi Variabel Polusi dengan Variabel Pemicu Polusi (Antar Parameter)**\n",
        "Dalam memahami korelasi antar parameter kita fokus pada indikator polusi yakni (PM2.5, PM10, SO2, NO2, CO dan O3) dan parameter pemicu polusi yakni (TEMP, PRES, DEWP, WSPM). Untuk yang pertama adalah indikator PM2.5, PM2.5 dapat dilihat pada matrix, tidak memiliki hubungan atau korelasi yang baik dengan indikator pemicu polusi hampir semua nilainya negatif atau kurang dari 1, ada satu parameter yang berkorelasi positif yakni DEWP akan tetapi korelasinya terlalu kecil sebesar 0.15. Ini juga terjadi pada PM10 yang hanya berkorelasi lemah dengan DEWP. Untuk SO2, NO2, dan CO berkorelasi lemah dengan PRES. Sedangkan O3 satu-satunya variabel yang menunjukkan korelasi dengan 2 parameter pemicu lumayan tinggi mendekati 1 pada TEMP dan berkorelasi lemah pada WSPM.\n",
        "\n",
        "##### **2. Pemahaman Korelasi Varibel Polusi atau Pemicu Polusi (Intra Parameter)**\n",
        "Dalam memahami korelasi intra-parameter kita fokus pada hubungan parameter satu dengan yang lainnya dalam satu kelompok yang sama. Hal yang menonjol disini adalah, nilai korelasi parameter O3 tidak menunjukkan hubungan/korelasi yang positif antaran sesama kelompok parameternya yakni PM2.5, PM10, SO2, NO2, dan CO. Korelasi parameter O3 bernilai negatif ini dapat dikatakan bahwa O3 tidak berkorelasi sama sekali dengan parameter polusi.\n",
        "\n",
        "Lalu jika dilihat pada parameter pemicu polusi, indikator TEMP berkorelasi kuat hanya pada indikator DEWP, sedangkan berkorelasi sangat lemah pada indikator WSPM. Indikator PRES bahkan pada matrix tidak memiliki korelasi yang kuat antar yang lainnya, hanya korelasi sangat lemah pada indikator WSPM. Untuk indikator DEWP, seperti yang dijelaskan, hanya berkorelasi tinggi pada indikator TEMP. Lalu terakhir, indikator WSPM hanya berkorelasi sangat lemah pada TEMP dan PRES."
      ],
      "metadata": {
        "id": "eg7BChAAE6ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üö• **Dashboard Data Visualization - Streamlit**"
      ],
      "metadata": {
        "id": "QTKs0SG8Uo-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Tahap Persiapan Data dan Streamlit untuk Visualisasi**\n",
        "\n",
        "Tahap persiapan ini akan memastikan alat-alat atau tool yang digunakan sudah baik seperti text editor dan lainnya. Selain itu pastikan sudah menjalankan kode pada analisis sebelumnya sehingga data sudah siap untuk dilakukan pemodelan dashboard.\n",
        "\n",
        "Untuk itu hal-hal yang disiapkan terlebih dahulu:\n",
        "\n",
        "1. DataFrame `final_df` yang menampung keseluruhan tabel sudah diubah menjadi .csv untuk memudahkan pengembanan dashboard. Untuk dataframe ini bisa dilakukan upload terlebih dahulu atay menjalankan kode keseluruhan analisis sebelumnya.\n",
        "\n",
        "2. Jika menggunakan platform cloud seperti colab maka perlu konfigurasi pihak ketiga seperti **ngrok** agar dapat diakses dalam proses pembuatannya, jika secara local maka cukup streamlit saja.\n",
        "\n",
        "3. Streamlit hanya berjalan pada ekstensi file .py bukan .ipynb jadi untuk dapat menggunakannya pada platform jupiter notebook bisa mengubah hasil kode menjadi .py terlebih dahulu, dengan `%%writefile nama_file.py`.\n",
        "\n",
        "4. Khusus untuk penggunaan ngrok sebagai pihak ketiga, diperlukan pembuatan akun dahulu untuk mendapatkan token authentifikasinya, lalu melakukan konfigurasi token `!ngrok authtoken kode_token`.\n",
        "\n",
        "5. Sebagai tambahan, jika sudah memiliki `final_df.csv`, `aqi-logo`, `app.py`, maka lakukan upload pada google colab dan silahkan langsung menuju ke Tahap 3 - Menjalankan Dasboard segmen *Start Server*.\n",
        "\n",
        "6. Tidak lupa untuk menjalankan kode pada *Segmen Library Data Analisis* sebelum memulai menjalankan Streamlit di goole colab."
      ],
      "metadata": {
        "id": "-W3I1cdaU9v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pyngrok"
      ],
      "metadata": {
        "id": "NqZ_G8UGUxRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authentification\n",
        "!ngrok authtoken kode_token"
      ],
      "metadata": {
        "id": "8Ye6EP41ViZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.read_csv('dashboard/final_df.csv')\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "5ZSufdcnW_CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Tahap Implementasi Streamlit**"
      ],
      "metadata": {
        "id": "KXP_RZnRXGh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dashboard/dashboard.py\n",
        "\n",
        "# Library Data Retrieving and Modification\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Library for Data Preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Library for Visualization\n",
        "from scipy.interpolate import interp1d\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "# Library for Dashboard\n",
        "import streamlit as st\n",
        "\n",
        "# \"\"\"\n",
        "# Segement 1 : Add Function for Visualization\n",
        "\n",
        "# This section will add a function that will use to processing the dataframe,\n",
        "# make visualization, and others.\n",
        "# \"\"\"\n",
        "\n",
        "def load_dataframe(filepath):\n",
        "    \"\"\"\n",
        "    Load and preprocess a DataFrame from a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path : str\n",
        "        The file path to the CSV file containing the DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    - pandas.DataFrame\n",
        "        A DataFrame sorted by 'datetime', with datetime columns converted.\n",
        "    \"\"\"\n",
        "\n",
        "    # Read the dataframe\n",
        "    all_df = pd.read_csv(filepath)\n",
        "\n",
        "    # Ensure the selected column is datetime format\n",
        "    datetime_columns = [\"datetime\"]\n",
        "\n",
        "    for column in datetime_columns:\n",
        "      all_df[column] = pd.to_datetime(all_df[column])\n",
        "\n",
        "    return all_df\n",
        "\n",
        "def filter_data_level(df, level):\n",
        "    \"\"\"\n",
        "    Filters the given DataFrame based on the specified levels (year, month, day).\n",
        "\n",
        "    This function dynamically displays Streamlit selectboxes for each filter level\n",
        "    (year, month, day) based on the provided `level` list, and filters the DataFrame\n",
        "    accordingly. The function assumes the DataFrame has columns `year`, `month`, and\n",
        "    `day` for filtering.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The input DataFrame containing the data to be filtered.\n",
        "    - level (list): A list containing the filtering levels. It can include 'year',\n",
        "      'month', and 'day'. The filtering occurs based on the order of the levels in the list.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The filtered DataFrame based on the selected levels.\n",
        "    \"\"\"\n",
        "\n",
        "    if 'year' in level:\n",
        "        selected_year = st.selectbox(\"Select Year:\", df['year'].unique())\n",
        "        df = df[df['year'] == selected_year]\n",
        "    if 'month' in level:\n",
        "        selected_month = st.selectbox(\"Select Month:\", df['month'].unique())\n",
        "        df = df[df['month'] == selected_month]\n",
        "    if 'day' in level:\n",
        "        selected_day = st.selectbox(\"Select Day:\", df['day'].unique())\n",
        "        df = df[df['day'] == selected_day]\n",
        "\n",
        "    return df\n",
        "\n",
        "def filter_selected_datetime(df, all_cols):\n",
        "    \"\"\"\n",
        "    Function to dynamically filter the DataFrame by year, month, day, and hour.\n",
        "\n",
        "    Parameters:\n",
        "    - spatial_df (pd.DataFrame): The dataframe containing datetime and spatial information.\n",
        "\n",
        "    Returns:\n",
        "    - selected_year (int or None): The selected year.\n",
        "    - selected_month (int or None): The selected month.\n",
        "    - selected_day (int or None): The selected day.\n",
        "    - selected_hour (int or None): The selected hour.\n",
        "    - filtered_df (pd.DataFrame): The filtered dataframe based on the selections.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize empty selections\n",
        "    selected_year = None\n",
        "    selected_month = None\n",
        "    selected_day = None\n",
        "    selected_hour = None\n",
        "\n",
        "    # Select Year\n",
        "    selected_year = all_cols[0].selectbox(\"Select Year:\", options=[None] + sorted(df['datetime'].dt.year.unique()))\n",
        "\n",
        "    # Select Month\n",
        "    if selected_year is not None:\n",
        "        available_months = sorted(df[df[\"datetime\"].dt.year == selected_year][\"datetime\"].dt.month.unique())\n",
        "    else:\n",
        "        available_months = []\n",
        "    selected_month = all_cols[1].selectbox(\"Select Month:\", options=[None] + available_months)\n",
        "\n",
        "    # Select Day\n",
        "    if selected_year is not None and selected_month is not None:\n",
        "        available_days = sorted(df[(df[\"datetime\"].dt.year == selected_year) & (df[\"datetime\"].dt.month == selected_month)][\"datetime\"].dt.day.unique())\n",
        "    else:\n",
        "        available_days=[]\n",
        "    selected_day = all_cols[2].selectbox(\"Select Day:\", options=[None] + available_days)\n",
        "\n",
        "    # Select Hour\n",
        "    if selected_year is not None and selected_month is not None and selected_day is not None:\n",
        "      available_hours = sorted(df[(df[\"datetime\"].dt.year == selected_year) & (df[\"datetime\"].dt.month == selected_month) & (df[\"datetime\"].dt.day == selected_day)][\"datetime\"].dt.hour.unique())\n",
        "    else:\n",
        "      available_hours=[]\n",
        "    selected_hour = all_cols[3].selectbox(\"Select Hour:\", options=[None] + available_hours)\n",
        "\n",
        "    # Apply the selected filters to the dataframe\n",
        "    filtered_df = df.copy()\n",
        "\n",
        "    # Filter Selection\n",
        "    if selected_year is not None:\n",
        "        filtered_df = filtered_df[filtered_df[\"datetime\"].dt.year == selected_year]\n",
        "    if selected_month is not None:\n",
        "        filtered_df = filtered_df[filtered_df[\"datetime\"].dt.month == selected_month]\n",
        "    if selected_day is not None:\n",
        "        filtered_df = filtered_df[filtered_df[\"datetime\"].dt.day == selected_day]\n",
        "    if selected_hour is not None:\n",
        "        filtered_df = filtered_df[filtered_df[\"datetime\"].dt.hour == selected_hour]\n",
        "\n",
        "    all_filter = [selected_year, selected_month, selected_day, selected_hour]\n",
        "\n",
        "    return all_filter, filtered_df\n",
        "\n",
        "def aggregate_data(df, visualization_type):\n",
        "    \"\"\"\n",
        "    Aggregates the given DataFrame based on the specified visualization type (Annual, Monthly, Daily, or Hourly).\n",
        "\n",
        "    This function computes aggregated statistics (mean) on air quality data, grouped by time-based components\n",
        "    such as year, month, day, or hour. The function can be customized to aggregate data for a specific year,\n",
        "    month, or day if necessary.\n",
        "\n",
        "    Parameters:\n",
        "    - df : pandas.DataFrame\n",
        "        The input DataFrame containing air quality data. Must include a 'datetime' column and pollutant data columns\n",
        "        (e.g., 'PM2.5', 'PM10').\n",
        "\n",
        "    - visualization_type : str\n",
        "        The type of aggregation to perform. Accepted values are:\n",
        "        - 'Annual' : Aggregates data by year.\n",
        "        - 'Monthly' : Aggregates data by month, optionally for a specific year.\n",
        "        - 'Daily' : Aggregates data by day, optionally for a specific month and year.\n",
        "        - 'Hours' : Aggregates data by hour, optionally for a specific day, month, and year.\n",
        "\n",
        "    - component_date : dict, optional (default is {'year': None, 'month': None, 'days': None})\n",
        "        A dictionary specifying the specific year, month, and day for monthly, daily, and hourly aggregations.\n",
        "        The dictionary may include:\n",
        "        - 'year' : The specific year for monthly, daily, or hourly aggregation.\n",
        "        - 'month' : The specific month for daily or hourly aggregation.\n",
        "        - 'days' : The specific day for hourly aggregation.\n",
        "        If not provided, the function will aggregate based on all available data.\n",
        "\n",
        "    Returns:\n",
        "    - agg_df : pandas.DataFrame\n",
        "        The aggregated DataFrame based on the chosen visualization type. The data is grouped by relevant time units\n",
        "        (e.g., year, month, day, or hour), and the mean of each pollutant is computed for each group.\n",
        "\n",
        "    - id_vars : list\n",
        "        A list of columns used as identifier variables for the aggregation. These columns depend on the selected\n",
        "        aggregation type (e.g., ['station', 'year'] for annual aggregation).\n",
        "\n",
        "    - labels : list\n",
        "        A list containing the name of the time unit used in the aggregation, along with its descriptive label\n",
        "        (e.g., ['year', 'Year']).\n",
        "\n",
        "    Raises:\n",
        "    - ValueError : If 'datetime' column is missing in the DataFrame.\n",
        "    - ValueError : If an invalid 'visualization_type' is provided.\n",
        "    - ValueError : If required values for year, month, or day are missing when performing monthly, daily, or hourly aggregation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Raise Value Error if datetime Column not present.\n",
        "    if 'datetime' not in df.columns:\n",
        "      raise ValueError(\"'datetime' column is missing in the DataFrame.\")\n",
        "\n",
        "    # Detect Year, Month, Day, and Hour\n",
        "    df.loc[:, 'year'] = df['datetime'].dt.year\n",
        "    df.loc[:, 'month'] = df['datetime'].dt.month\n",
        "    df.loc[:, 'day'] = df['datetime'].dt.day\n",
        "    df.loc[:, 'hour'] = df['datetime'].dt.hour\n",
        "\n",
        "    # Initialize variables\n",
        "    agg_df = None\n",
        "    id_vars = []\n",
        "    labels = []\n",
        "\n",
        "    # Use match-case for aggregation\n",
        "    match visualization_type:\n",
        "      case 'Annual':\n",
        "          agg_df = df.groupby(['station', 'year']).mean().reset_index()\n",
        "          id_vars = ['station', 'year']\n",
        "          labels = ['year', 'Year']\n",
        "          agg_df.drop(columns=['month', 'day', 'hour'], inplace=True)\n",
        "\n",
        "      case 'Monthly':\n",
        "          if df['year'].nunique() > 1:\n",
        "              df = filter_data_level(df, ['year'])\n",
        "          agg_df = df.groupby(['station', 'month']).mean().reset_index()\n",
        "          id_vars = ['station', 'month']\n",
        "          labels = ['month', 'Month']\n",
        "          agg_df.drop(columns=['year', 'day', 'hour'], inplace=True)\n",
        "\n",
        "      case 'Daily':\n",
        "          if df['month'].nunique() > 1:\n",
        "              df = filter_data_level(df, ['year', 'month'])\n",
        "          agg_df = df.groupby(['station', 'day']).mean().reset_index()\n",
        "          id_vars = ['station', 'day']\n",
        "          labels = ['day', 'Day']\n",
        "          agg_df.drop(columns=['month', 'year', 'hour'], inplace=True)\n",
        "\n",
        "      case 'Hours':\n",
        "          if df['day'].nunique() > 1:\n",
        "              df = filter_data_level(df, ['year', 'month', 'day'])\n",
        "          agg_df = df.groupby(['station', 'hour']).mean().reset_index()\n",
        "          id_vars = ['station', 'hour']\n",
        "          labels = ['hour', 'Hour']\n",
        "          agg_df.drop(columns=['month', 'day', 'year'], inplace=True)\n",
        "\n",
        "      case _:\n",
        "          raise ValueError(\"Invalid visualization type. Choose from 'Annual', 'Monthly', 'Daily', 'Hours'.\")\n",
        "\n",
        "    return agg_df, id_vars, labels\n",
        "\n",
        "def ploty_line_visualization(data, id_vars, labels, parameters, title_template=\"Average Concentration Data by Station\"):\n",
        "    \"\"\"\n",
        "    Creates a Plotly line chart to visualize air quality data by station and a specified time unit (e.g., year, month, hour).\n",
        "\n",
        "    Parameters:\n",
        "    - data : pandas.DataFrame\n",
        "        The input data containing station names, time variables, and pollutant concentrations.\n",
        "        The DataFrame must include columns corresponding to the time unit(s), pollutant concentrations, and station identifiers.\n",
        "\n",
        "    - id_vars : list\n",
        "        The identifier variables used to group the data (e.g., ['station', 'year']).\n",
        "\n",
        "    - labels : list\n",
        "        A list containing the name of the time unit column and its descriptive label (e.g., ['year', 'Year']).\n",
        "\n",
        "    - parameters : list\n",
        "        A list of pollutant columns to include in the visualization (e.g., ['PM2.5', 'PM10']).\n",
        "\n",
        "    - title_template : str, optional (default: \"Concentration Data by Station and {}\")\n",
        "        A string template for the chart title, with the placeholder `{}` being replaced by the descriptive label of the time unit.\n",
        "\n",
        "    Returns:\n",
        "    - plotly.graph_objects.Figure\n",
        "        A Plotly figure object representing the line chart, visualizing the pollutant concentrations by station over the specified time unit(s).\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if the dataframe empty or not\n",
        "    if data.empty:\n",
        "      raise ValueError(\"The data provided is empty. Please check the aggregation process.\")\n",
        "\n",
        "    # For Better Visualization will Using Long Format with Function melt()\n",
        "    df_melted = data.melt(\n",
        "        id_vars=id_vars,\n",
        "        value_vars=parameters,\n",
        "        var_name='Pollutant',\n",
        "        value_name='Concentration'\n",
        "    )\n",
        "\n",
        "    # Define Free Variabel to Set the Visualization Plot\n",
        "    if 'month' in df_melted.columns:\n",
        "        month_names = {\n",
        "            1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',\n",
        "            7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'\n",
        "        }\n",
        "        df_melted['month_name'] = df_melted['month'].map(month_names)\n",
        "        labels[0] = 'month_name'\n",
        "        tick_mode = 'array'\n",
        "        tick_val = list(month_names.values())\n",
        "        tick_text = tick_val\n",
        "        tick_angle = 45\n",
        "    elif 'hour' in df_melted.columns:\n",
        "        df_melted['AM_PM'] = df_melted['hour'].apply(lambda x: f\"{x % 12 or 12} {'AM' if x < 12 else 'PM'}\")\n",
        "        labels[0] = 'AM_PM'\n",
        "        tick_mode = 'array'\n",
        "        tick_val = df_melted['AM_PM'].unique()\n",
        "        tick_text = tick_val\n",
        "        tick_angle = 90\n",
        "    else:\n",
        "        tick_mode = 'linear'\n",
        "        tick_val = None\n",
        "        tick_text = None\n",
        "        tick_angle = None\n",
        "\n",
        "    fig = px.line(\n",
        "        df_melted,\n",
        "        x=f\"{labels[0]}\",\n",
        "        y='Concentration',\n",
        "        color='station',\n",
        "        labels={labels[0]: labels[1], 'Concentration': 'Concentration', 'station': 'Station'},\n",
        "        line_shape='linear'\n",
        "    )\n",
        "\n",
        "    # Set the Layout Visualization\n",
        "    fig.update_layout(\n",
        "        title=title_template,\n",
        "        legend_title='Station',\n",
        "        template='simple_white',\n",
        "        width=900,\n",
        "        height=450,\n",
        "        title_font=dict(size=16),\n",
        "        xaxis=dict(\n",
        "            title=\"\",\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12),\n",
        "            showgrid=False,\n",
        "            tickmode=tick_mode,\n",
        "            tickformat=\".0f\",\n",
        "            tickvals=tick_val,\n",
        "            ticktext=tick_text,\n",
        "            tickangle=tick_angle\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            title=\"\",\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12),\n",
        "            showgrid=True,\n",
        "            gridcolor='lightgrey'\n",
        "        ),\n",
        "        legend=dict(\n",
        "            groupclick='toggleitem',\n",
        "            title='Station',\n",
        "            font=dict(size=12),\n",
        "        ),\n",
        "        hovermode='closest',\n",
        "        plot_bgcolor='white',\n",
        "        paper_bgcolor='ghostwhite'\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "def ploty_bar_visualization(data, id_vars, labels, parameters, title_template=\"Average Concentration Data by Station\"):\n",
        "    \"\"\"\n",
        "    Visualizes pollutant concentration data in a stacked bar plot, with dynamic hover information\n",
        "    based on the type of time-related data available (month, year, day, or AM/PM).\n",
        "\n",
        "    Parameters:\n",
        "    - data : pandas.DataFrame\n",
        "        The input dataframe containing the data to be visualized.\n",
        "\n",
        "    - id_vars : list of str\n",
        "        The columns in the dataframe that will be used for grouping the x-axis (e.g., station name).\n",
        "\n",
        "    - labels : list of str\n",
        "        A list where the first item is the column used for coloring the bars (e.g., 'year')\n",
        "        and the second item is the label for that column in the plot.\n",
        "\n",
        "    - parameters : list of str\n",
        "        A list containing the names of the pollutant parameters (e.g., ['PM2.5']) to be plotted.\n",
        "\n",
        "    - title_template : str, optional, default=\"Average Concentration Data by Station and {}\"\n",
        "        The template for the plot's title. The `{}` placeholder will be replaced with the second\n",
        "        item in the `labels` list.\n",
        "\n",
        "    Returns:\n",
        "    - plotly.graph_objects.Figure\n",
        "        The Plotly figure object containing the bar plot visualization.\n",
        "    \"\"\"\n",
        "\n",
        "    # Default hover customiation\n",
        "    custom_value = 'datetime'\n",
        "    hover_temp = \"Date: %{customdata[0]}<br>\"\n",
        "\n",
        "    # Customize Month Column and Hover\n",
        "    if 'month' in data.columns:\n",
        "      month_names = {\n",
        "          1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',\n",
        "          7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'\n",
        "      }\n",
        "      data['Month'] = data['month'].map(month_names)\n",
        "      custom_value = 'Month'\n",
        "      hover_temp = \"Month: %{customdata[0]}<br>\"\n",
        "\n",
        "    # Customize Year Column and Hover\n",
        "    elif 'year' in data.columns:\n",
        "      custom_value = 'year'\n",
        "      hover_temp= \"Year: %{customdata[0]}<br>\"\n",
        "\n",
        "    # Customize Day Column and Hover\n",
        "    elif 'day' in data.columns:\n",
        "      custom_value = 'day'\n",
        "      hover_temp= \"Day: %{customdata[0]}<br>\"\n",
        "\n",
        "    # Customize Clocl Column and Hover\n",
        "    else:\n",
        "      data['AM_PM'] = data['hour'].apply(lambda x: f\"{x % 12 or 12} {'AM' if x < 12 else 'PM'}\")\n",
        "      custom_value = 'AM_PM'\n",
        "      hover_temp= \"Hour: %{customdata[0]}<br>\"\n",
        "\n",
        "    # Do Sorting Data by Parameter\n",
        "    data = data.sort_values(by=parameters[0])\n",
        "\n",
        "    # Wrap All Variabel Visualization\n",
        "    fig = px.bar(\n",
        "        data,\n",
        "        x=id_vars[0],\n",
        "        y=parameters,\n",
        "        color=id_vars[1],\n",
        "        barmode='stack',\n",
        "        hover_data=None,\n",
        "    )\n",
        "\n",
        "    # Customize Hover\n",
        "    fig.update_traces(customdata=data[[custom_value]].values)\n",
        "    fig.update_traces(\n",
        "        hovertemplate=\"<b>%{x}</b><br>\"\n",
        "        + \"Concentration: %{y:.2f}<br>\"\n",
        "        + hover_temp\n",
        "    )\n",
        "\n",
        "    # Set the Layout Visualization\n",
        "    fig.update_layout(\n",
        "        title=title_template,\n",
        "        legend_title='Station',\n",
        "        template='simple_white',\n",
        "        width=900,\n",
        "        height=450,\n",
        "        title_font=dict(size=16),\n",
        "        xaxis=dict(\n",
        "            title=\"\",\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12),\n",
        "            showgrid=False,\n",
        "            tickformat=\".0f\",\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            title=\"\",\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12),\n",
        "            showgrid=True,\n",
        "            gridwidth=1,\n",
        "            gridcolor='lightgrey'\n",
        "        ),\n",
        "        legend=dict(\n",
        "            title='Station',\n",
        "            font=dict(size=12),\n",
        "        ),\n",
        "        hovermode='closest',\n",
        "        plot_bgcolor='white',\n",
        "        paper_bgcolor='ghostwhite'\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "def ploty_geospatial_visualization(filtered_df, selected_filters, parameter):\n",
        "    \"\"\"\n",
        "    Generates a geospatial visualization (scatter plot on a map) based on the given DataFrame\n",
        "    and selected filters. The plot displays pollutant data based on the selected year, month,\n",
        "    day, and hour, with additional data about the stations.\n",
        "\n",
        "    The function filters the DataFrame based on the selected filters (year, month, day, hour),\n",
        "    groups the data by the relevant columns, and visualizes it using Plotly's scatter_mapbox\n",
        "    function.\n",
        "\n",
        "    Parameters:\n",
        "    - filtered_df (pd.DataFrame): The input DataFrame containing the data to be visualized.\n",
        "    - selected_filters (list): A list of selected filters for the visualization in the order\n",
        "      [selected_year, selected_month, selected_day, selected_hour].\n",
        "    - parameter (list): A list containing the column name of the pollutant (e.g., [\"PM2.5\"]).\n",
        "\n",
        "    Returns:\n",
        "    - fig (plotly.graph_objs.Figure): The Plotly figure object containing the geospatial\n",
        "      visualization.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract selected filters\n",
        "    selected_year, selected_month, selected_day, selected_hour = selected_filters\n",
        "\n",
        "    # Define Group Column and Hover for Visualization\n",
        "    group_cols = [\"station\", \"latitude\", \"longitude\"]\n",
        "    hover_cols = [parameter[0]]\n",
        "\n",
        "    if selected_hour is not None:\n",
        "        filtered_df[\"hour\"] = filtered_df[\"datetime\"].dt.hour\n",
        "        group_cols = [\"hour\"] + group_cols\n",
        "        hover_cols = [\"hour\", parameter[0]]\n",
        "    elif selected_day is not None:\n",
        "        filtered_df[\"day\"] = filtered_df[\"datetime\"].dt.day\n",
        "        group_cols = [\"day\"] + group_cols\n",
        "        hover_cols = [\"day\", parameter[0]]\n",
        "    elif selected_month is not None:\n",
        "        filtered_df[\"month\"] = filtered_df[\"datetime\"].dt.month\n",
        "        group_cols = [\"month\"] + group_cols\n",
        "        hover_cols = [\"month\", parameter[0]]\n",
        "    elif selected_year is not None:\n",
        "        filtered_df[\"year\"] = filtered_df[\"datetime\"].dt.year\n",
        "        group_cols = [\"year\"] + group_cols\n",
        "        hover_cols = [\"year\", parameter[0]]\n",
        "\n",
        "    # Handle missing columns or no filters applied\n",
        "    if len(group_cols) == 3:\n",
        "        hover_cols = [parameter[0]]\n",
        "\n",
        "    # Sort the Filter Dataframe\n",
        "    filtered_df = (\n",
        "        filtered_df.groupby(group_cols)\n",
        "        .agg({parameter[0]: \"mean\"})\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Do Visualization\n",
        "    fig = px.scatter_mapbox(\n",
        "        filtered_df,\n",
        "        lat=\"latitude\",\n",
        "        lon=\"longitude\",\n",
        "        color=parameter[0],\n",
        "        size=parameter[0],\n",
        "        hover_name=\"station\",\n",
        "        hover_data=hover_cols,\n",
        "        mapbox_style=\"carto-positron\",\n",
        "        title=\"Geospatial Visualization Polutan Data\",\n",
        "        zoom=5,\n",
        "    )\n",
        "\n",
        "    # Adjust layout to remove gaps and specify the figure height\n",
        "    fig.update_layout(\n",
        "        height=400,\n",
        "        margin={\"r\": 100, \"t\": 50, \"l\": 0, \"b\": 0},  # Adjust margin\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "def check_correlation(df):\n",
        "    \"\"\"\n",
        "    Plots a correlation matrix heatmap for the numeric columns of a DataFrame.\n",
        "\n",
        "    This function calculates the Pearson correlation coefficient between each\n",
        "    pair of numeric columns, excluding specified non-numeric columns, and\n",
        "    visualizes the correlation matrix as a heatmap. Correlation coefficients\n",
        "    range from -1 to 1, with values closer to 1 or -1 indicating stronger correlations.\n",
        "    - Positive values indicate a positive correlation.\n",
        "    - Negative values indicate a negative correlation.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The input DataFrame containing numeric and non-numeric data.\n",
        "\n",
        "    Returns:\n",
        "    - None: This function only displays the heatmap and does not return any values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Drop column that not will use in make correlation matrix\n",
        "    df_cor = df.drop(columns=['station', 'wd', 'datetime', 'RAIN'])\n",
        "\n",
        "    # Get correlation value and store in correlation variabel\n",
        "    correlation_matrix = df_cor.corr()\n",
        "\n",
        "    # Visualize the correlation value into correlation matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Correlation Matrix Heatmap')\n",
        "    st.pyplot(plt)\n",
        "\n",
        "\n",
        "\n",
        "# \"\"\"\n",
        "# Segment 2 : Make Filter Layout in Side Bar\n",
        "\n",
        "# This section will make filter layout in side bar using streamlit method siderbar.\n",
        "# for the input is start date and end date. That will pass to new dataframe main_df\n",
        "# that contain data from selected date\n",
        "# \"\"\"\n",
        "\n",
        "# Load DataFrame and Define Variabel for Sidebar\n",
        "final_df = load_dataframe('dashboard/final_df.csv')\n",
        "p_polution_final_df = final_df.drop(columns=['TEMP','PRES','DEWP','RAIN','wd','WSPM'])\n",
        "\n",
        "# Make Filter Layout in Side Bar\n",
        "with st.sidebar:\n",
        "\n",
        "  # Set image logo\n",
        "  st.image(\"data/aqi-logo.png\")\n",
        "\n",
        "  # Set start_date and end_date from input\n",
        "  p_polution_final_df[\"date\"] = pd.to_datetime(p_polution_final_df[\"datetime\"]).dt.date\n",
        "  min_date = final_df[\"datetime\"].min()\n",
        "  max_date = final_df[\"datetime\"].max()\n",
        "\n",
        "  selected_dates = st.date_input(\n",
        "      label=\"Rentang Waktu\",\n",
        "      min_value=min_date,\n",
        "      max_value=max_date,\n",
        "      value=[min_date, max_date]\n",
        "  )\n",
        "\n",
        "  # If only one date is selected, set start_date and end_date to the same date\n",
        "  if len(selected_dates) == 1:\n",
        "      start_date = selected_dates[0]\n",
        "      end_date = selected_dates[0]\n",
        "  else:\n",
        "      start_date, end_date = selected_dates\n",
        "\n",
        "# Define dataframe filter\n",
        "main_df = p_polution_final_df[(p_polution_final_df[\"date\"] >= start_date) & (p_polution_final_df[\"date\"] <= end_date)]\n",
        "main_df = main_df.drop(columns=[\"date\"])\n",
        "\n",
        "\n",
        "\n",
        "# \"\"\"\n",
        "# Segment 3 : Define the Variabel That will Use for Dashboard Visualization\n",
        "\n",
        "# This section will add some visulization that will use in dashboard that also correlated\n",
        "# with business question. The visualization that will use wil same as the section\n",
        "# Data Visualization before.\n",
        "# \"\"\"\n",
        "\n",
        "# Initialize Header Dashboard\n",
        "st.header(\"Dicoding Air Quality Dashboard üÜò\")\n",
        "\n",
        "# Variabel for Visualization\n",
        "st.subheader(\"Select Parameter and Type Visualization\")\n",
        "parameter = st.radio(\n",
        "    \"Choose Parameter Visualization:\",\n",
        "    [\"PM2.5\", \"PM10\", \"SO2\", \"NO2\", \"CO\", \"O3\"],\n",
        "    horizontal=True\n",
        ")\n",
        "\n",
        "visualization_type = st.radio(\n",
        "    \"Choose Visualization Type:\",\n",
        "    [\"Annual\", \"Monthly\", \"Daily\", \"Hours\"],\n",
        "    horizontal=True\n",
        ")\n",
        "\n",
        "title= f\"Trend of {parameter} - {visualization_type} Aggregation\"\n",
        "title_bar= f\"Best and Worst of {parameter} - {visualization_type} Aggregation\"\n",
        "\n",
        "# Visualize 1 : Make Line Visualization for Trend Pollution Parameter\n",
        "st.subheader(\"Trend Polution Parameter üìà\")\n",
        "st.markdown(\"This chart shows the trend of the selected air quality parameter over time.\")\n",
        "\n",
        "agg_df, id_vars, labels = aggregate_data(main_df, visualization_type)\n",
        "fig = ploty_line_visualization(agg_df, id_vars, labels, [parameter], title_template=title)\n",
        "st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "explanation = \"\"\"\n",
        "- Visualisasi dengan menggunakan line plot dapat melihat trend data baik dalam kategori Tahunan, Bulanan, Harian, dan bahkan Jam.\n",
        "- Dalam menggunakannya pastikan sudah menetapkan rentangan tanggal dan memilih tipe visualisasi yang sesuai.\n",
        "- Jika rentangan tahunan, maka visualisasi yang dapat ditampilkan adalah trend antar tahun, bulan, hari, dan jam.\n",
        "- Sebaliknya jika rentangan hanya 1 bulan, maka visualisasi yang muncul nanti hanya rentangan hari, dan juga bisa memilih rentangan jam.\n",
        "\"\"\"\n",
        "\n",
        "with st.expander(\"Explanation: \"):\n",
        "    st.markdown(explanation)\n",
        "\n",
        "# Visualize 2: Make Bar Visualization for Show Best and Worst Year, Month, Day, and Hours in Polution Parameter\n",
        "st.subheader(\"Best and Worst Polutan Parameter According to Datetime üìä \")\n",
        "st.markdown(\"This bar chart shows the worst and best selected air quality parameter in a time.\")\n",
        "\n",
        "bar_fig = ploty_bar_visualization(agg_df, id_vars, labels, [parameter], title_template=title_bar)\n",
        "st.plotly_chart(bar_fig, use_container_width=True)\n",
        "\n",
        "explanation = \"\"\"\n",
        "- Penggunaan visualisasi bar plot ini menunjukkan perbandingan data baik dalam tahun, bulan, hari, dan jam.\n",
        "- Jika tidak ada data yang dibandingkan maka visualisasi akan menggunakan data yang ada. Sebagai contoh:\n",
        "  - Jika memilih rentang nilai dalam satu bulan, maka jika memilih tipe visualisasi 'Annual'\n",
        "    visualisasi akan tetap jalan dengan data keseluruhan di tahun itu.\n",
        "  - Ini sama dengan jika memilih tipe visualisasi 'Monthly' maka tetap menampilkan visualisasi hanya pada bulan itu saja.\n",
        "  - Akan tetapi jika memlih tipe visualisasi 'Daily' atau 'Hourly' baru akan ada nilai yang dibandingkan.\n",
        "- Nilai pada data sudah dilakukan proses sorting jadi data teratas adalah data dengan konsentrasi tertinggi\n",
        "  baik pada tipe Tahun, Bulan, Hari, atau Jam.\n",
        "\"\"\"\n",
        "\n",
        "with st.expander(\"Explanation: \"):\n",
        "    st.markdown(explanation)\n",
        "\n",
        "# Visualize 3: Make Geospasial Visualization for Area of Polution\n",
        "st.subheader(\"Area of Polution üåç\")\n",
        "st.markdown(\"This map shows the distribution of the selected air quality parameter across different locations.\")\n",
        "\n",
        "stations_coords = {\n",
        "    \"Aotizhongxin\": (41.731242, 123.456778),\n",
        "    \"Changping\": (40.221, 116.2312),\n",
        "    \"Dingling\": (40.28998423518348, 116.2393424781757),\n",
        "    \"Dongsi\": (40.10208908941478, 116.31657335910373),\n",
        "    \"Guanyuan\": (39.94113871141321, 116.3610710753842),\n",
        "    \"Gucheng\": (39.91270053243136, 116.1868698799306),\n",
        "    \"Huairou\": (43.06043347888646, 117.46726428196578),\n",
        "    \"Nongzhanguan\": (39.93978579546827, 116.46859787734736),\n",
        "    \"Shunyi\": (40.151287025024715, 116.69280368021326),\n",
        "    \"Tiantan\": (39.88189413732897, 116.42047003643812),\n",
        "    \"Wanliu\": (39.99843210685499, 116.25774299569612),\n",
        "    \"Wanshouxigong\": (39.90816416629832, 116.26439549963654)\n",
        "}\n",
        "\n",
        "spatial_df = final_df.copy()\n",
        "spatial_df['date'] = pd.to_datetime(spatial_df[\"datetime\"]).dt.date\n",
        "spatial_df = spatial_df[(spatial_df[\"date\"] >= start_date) & (spatial_df[\"date\"] <= end_date)]\n",
        "spatial_df.drop(columns=[\"date\"])\n",
        "\n",
        "spatial_df[\"latitude\"] = spatial_df[\"station\"].map(lambda x: stations_coords[x][0])\n",
        "spatial_df[\"longitude\"] = spatial_df[\"station\"].map(lambda x: stations_coords[x][1])\n",
        "spatial_df[\"datetime\"] = pd.to_datetime(spatial_df[\"datetime\"])\n",
        "\n",
        "st.markdown(\"#### Select Geospasial Datetime\")\n",
        "col1, col2, col3, col4 = st.columns([1, 1, 1, 1])\n",
        "\n",
        "all_cols = [col1, col2, col3, col4]\n",
        "all_filters, filtered_df = filter_selected_datetime(spatial_df, all_cols)\n",
        "\n",
        "fig = ploty_geospatial_visualization(filtered_df, all_filters, [parameter])\n",
        "st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "explanation = \"\"\"\n",
        "- Ini adalah persebaran parameter polusi sesuai dengan latitude dan longitude-nya.\n",
        "- Dalam persebaran ini diharuskan memilih persebaran sesuai dengan tahun, lalu bulan, hari, dan waktunya.\n",
        "- Jika peta persebaran tidak langsung menunjukkan lokasinya maka bisa dikatakan dalam waktu itu tidak ada\n",
        "data persebaran yang tersedia. Sebagai contoh:\n",
        "  - Memilih tanggal 1 Januari 2023 tidak akan memunculkan nilai persebaran peta. Karena tidak ada data pada tanggal tersebut.\n",
        "\"\"\"\n",
        "\n",
        "with st.expander(\"Explanation: \"):\n",
        "    st.markdown(explanation)\n",
        "\n",
        "# Visualize 4: Make Matrix Correlation for Parameter in Air Quality Index\n",
        "st.subheader(\"Matrix Correlation Polutan Parameter üíê\")\n",
        "st.markdown(\"This matrix shows the correlation between different air quality parameters.\")\n",
        "\n",
        "check_correlation(final_df)\n",
        "\n",
        "explanation = \"\"\"\n",
        "- Korelasi adalah ukuran statistik yang menunjukkan sejauh mana dua variabel beruhubungan.\n",
        "- Nilai koefisien korelasi berkisar dari -1 hingga 1:\n",
        "  - **1** menunjukkan korelasi positif sempurna (kedua variabel meningkat bersama).\n",
        "  - **-1** menunjukkan korelasi negatif sempurna (saat satu variabel meningkat, variabel lainnya menurun).\n",
        "  - **0** menunjukkan tidak ada korelasi antara variabel-variabel tersebut.\n",
        "- Pada heatmap ini, warna yang lebih gelap mewakili korelasi yang lebih kuat, dengan warna biru menunjukkan korelasi negatif dan merah menunjukkan korelasi positif.\n",
        "- Korelasi positif menunjukkan bahwa saat satu variabel meningkat, yang lainnya cenderung meningkat juga, dan sebaliknya untuk korelasi negatif.\n",
        "\"\"\"\n",
        "\n",
        "with st.expander(\"Explanation: \"):\n",
        "    st.markdown(explanation)\n"
      ],
      "metadata": {
        "id": "Gi6ROp0pXOL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Tahap Menjalankan Dasboard**"
      ],
      "metadata": {
        "id": "D5jf5b-osDvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üèÉüèª‚Äç‚ôÄÔ∏è **Start Server**"
      ],
      "metadata": {
        "id": "qB7X3JANtJTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_streamlit_app(app_name, port=8501):\n",
        "    \"\"\"\n",
        "    Runs a Streamlit app and sets up an ngrok tunnel to expose it to the internet.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    app_name : str\n",
        "        The name of the Streamlit app file (e.g., \"app.py\").\n",
        "    port : int, optional, default=8501\n",
        "        The port on which to run the Streamlit app (default is 8501).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        The public URL provided by ngrok for the Streamlit app.\n",
        "    \"\"\"\n",
        "    # Kill any previous tunnels\n",
        "    ngrok.kill()\n",
        "\n",
        "    # Run the Streamlit app\n",
        "    subprocess.Popen([\"streamlit\", \"run\", app_name, \"--server.port\", str(port)])\n",
        "\n",
        "    # Start a new ngrok tunnel\n",
        "    ngrok_tunnel = ngrok.connect(port, \"http\")\n",
        "\n",
        "    # Print the public URL for the Streamlit app\n",
        "    print(\"Streamlit URL:\", ngrok_tunnel.public_url)\n",
        "\n",
        "    return ngrok_tunnel.public_url"
      ],
      "metadata": {
        "id": "51I6H3wvsPwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "app_name = \"dashboard/dashboard.py\"\n",
        "run_streamlit_app(app_name)"
      ],
      "metadata": {
        "id": "sWe6T6pKsWeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ùå **Shutdown Server**\n"
      ],
      "metadata": {
        "id": "DiIB3go7scsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep ngrok"
      ],
      "metadata": {
        "id": "YCtcolxasknn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "riA0rapjspFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep ngrok"
      ],
      "metadata": {
        "id": "8ghe9uM-sqsf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}